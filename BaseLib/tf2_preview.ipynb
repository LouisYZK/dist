{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n",
      "2.2.4-tf\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "print(tf.__version__)\n",
    "print(tf.keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## keras 堆叠模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(layers.Dense(32, activation='relu'))\n",
    "model.add(layers.Dense(32, activation='relu'))\n",
    "model.add(layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "              loss=tf.keras.losses.categorical_crossentropy,\n",
    "              metrics=[tf.keras.metrics.categorical_accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples, validate on 200 samples\n",
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 0s 475us/sample - loss: 11.9318 - categorical_accuracy: 0.1040 - val_loss: 12.0681 - val_categorical_accuracy: 0.1050\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 0s 30us/sample - loss: 12.4159 - categorical_accuracy: 0.1040 - val_loss: 12.9323 - val_categorical_accuracy: 0.0850\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 0s 29us/sample - loss: 13.6672 - categorical_accuracy: 0.0870 - val_loss: 14.7215 - val_categorical_accuracy: 0.0750\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 0s 29us/sample - loss: 15.9045 - categorical_accuracy: 0.0970 - val_loss: 17.6465 - val_categorical_accuracy: 0.1000\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 0s 29us/sample - loss: 19.3389 - categorical_accuracy: 0.1080 - val_loss: 21.6197 - val_categorical_accuracy: 0.1050\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 0s 29us/sample - loss: 23.4647 - categorical_accuracy: 0.1040 - val_loss: 25.8203 - val_categorical_accuracy: 0.0950\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 0s 29us/sample - loss: 28.2279 - categorical_accuracy: 0.1070 - val_loss: 31.5120 - val_categorical_accuracy: 0.0950\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 0s 27us/sample - loss: 34.7599 - categorical_accuracy: 0.1050 - val_loss: 38.9985 - val_categorical_accuracy: 0.0900\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 0s 28us/sample - loss: 42.6738 - categorical_accuracy: 0.0970 - val_loss: 47.4111 - val_categorical_accuracy: 0.0900\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 0s 29us/sample - loss: 51.7570 - categorical_accuracy: 0.0990 - val_loss: 56.8850 - val_categorical_accuracy: 0.0950\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fba90523278>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "train_x = np.random.random((1000, 72))\n",
    "train_y = np.random.random((1000, 10))\n",
    "val_x = np.random.random((200, 72))\n",
    "val_y = np.random.random((200, 10))\n",
    "model.fit(train_x, train_y, epochs=10, batch_size=100,\n",
    "          validation_data=(val_x, val_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 38us/sample - loss: 56.3595 - categorical_accuracy: 0.0890\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[56.35947882080078, 0.089]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x = np.random.random((1000, 72))\n",
    "test_y = np.random.random((1000, 10))\n",
    "\n",
    "model.evaluate(test_x, test_y, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.13609815e-06, 4.39963710e-09, 6.00056246e-13, ...,\n",
       "        2.10448071e-01, 1.08942135e-04, 3.01993615e-03],\n",
       "       [5.94892617e-06, 4.09629379e-08, 1.05548677e-11, ...,\n",
       "        2.24327385e-01, 1.72162268e-04, 6.30125776e-03],\n",
       "       [8.56924089e-06, 6.37576960e-08, 1.22256727e-11, ...,\n",
       "        1.98053017e-01, 2.16444823e-04, 7.91333150e-03],\n",
       "       ...,\n",
       "       [1.02347167e-05, 5.06844096e-08, 3.48560694e-11, ...,\n",
       "        4.26875442e-01, 3.14823788e-04, 3.88961844e-03],\n",
       "       [1.26333771e-06, 3.55623397e-09, 2.52151165e-13, ...,\n",
       "        1.45974874e-01, 1.12772497e-04, 2.32446170e-03],\n",
       "       [4.22254516e-06, 2.99279108e-08, 7.45401223e-12, ...,\n",
       "        3.53763252e-01, 8.36234249e-05, 8.29309784e-03]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## keras函数式API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1000/1000 [==============================] - 0s 242us/sample - loss: 12.3133 - accuracy: 0.1060\n",
      "Epoch 2/5\n",
      "1000/1000 [==============================] - 0s 67us/sample - loss: 17.5539 - accuracy: 0.1070\n",
      "Epoch 3/5\n",
      "1000/1000 [==============================] - 0s 67us/sample - loss: 32.0443 - accuracy: 0.0990\n",
      "Epoch 4/5\n",
      "1000/1000 [==============================] - 0s 64us/sample - loss: 60.1526 - accuracy: 0.1020\n",
      "Epoch 5/5\n",
      "1000/1000 [==============================] - 0s 60us/sample - loss: 102.4671 - accuracy: 0.0970\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fba780ec358>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_x = tf.keras.Input(shape=(72,))\n",
    "hidden1 = layers.Dense(32, activation='relu')(input_x)\n",
    "hidden2 = layers.Dense(16, activation='relu')(hidden1)\n",
    "pred = layers.Dense(10, activation='softmax')(hidden2)\n",
    "\n",
    "model = tf.keras.Model(inputs=input_x, outputs=pred)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "              loss=tf.keras.losses.categorical_crossentropy,\n",
    "              metrics=['accuracy'])\n",
    "model.fit(train_x, train_y, batch_size=32, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型子类化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 0s 299us/sample - loss: 13.6494 - accuracy: 0.1050\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 0s 121us/sample - loss: 17.9146 - accuracy: 0.0980\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 0s 119us/sample - loss: 21.7370 - accuracy: 0.0990\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 0s 117us/sample - loss: 24.3099 - accuracy: 0.0870\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 0s 115us/sample - loss: 26.4952 - accuracy: 0.0850\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 0s 113us/sample - loss: 27.0150 - accuracy: 0.0980\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 0s 112us/sample - loss: 26.3332 - accuracy: 0.0850\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 0s 112us/sample - loss: 25.3105 - accuracy: 0.0940\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 0s 115us/sample - loss: 24.6762 - accuracy: 0.0910\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 0s 110us/sample - loss: 24.7764 - accuracy: 0.0870\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fba107d3da0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(MyModel, self).__init__(name='my_model')\n",
    "        self.num_classes = num_classes\n",
    "        self.layer1 = layers.Dense(32, activation='relu')\n",
    "        self.layer2 = layers.Dense(num_classes, activation='softmax')\n",
    "    def call(self, inputs):\n",
    "        h1 = self.layer1(inputs)\n",
    "        out = self.layer2(h1)\n",
    "        return out\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        shape = tf.TensorShape(input_shape).as_list()\n",
    "        shape[-1] = self.num_classes\n",
    "        return tf.TensorShape(shape)\n",
    "\n",
    "model = MyModel()\n",
    "model.compile(optimizer=tf.keras.optimizers.RMSprop(0.001),\n",
    "              loss=tf.keras.losses.categorical_crossentropy,\n",
    "              metrics=['accuracy'])\n",
    "model.fit(train_x, train_y, batch_size=16, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 回调"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples, validate on 200 samples\n",
      "Epoch 1/5\n",
      "1000/1000 [==============================] - 0s 155us/sample - loss: 28.6291 - accuracy: 0.0900 - val_loss: 28.0314 - val_accuracy: 0.0800\n",
      "Epoch 2/5\n",
      "1000/1000 [==============================] - 0s 135us/sample - loss: 28.9576 - accuracy: 0.0850 - val_loss: 27.8878 - val_accuracy: 0.0800\n",
      "Epoch 3/5\n",
      "1000/1000 [==============================] - 0s 136us/sample - loss: 29.0269 - accuracy: 0.0830 - val_loss: 28.9413 - val_accuracy: 0.1000\n",
      "Epoch 4/5\n",
      "1000/1000 [==============================] - 0s 136us/sample - loss: 29.5299 - accuracy: 0.0820 - val_loss: 29.1710 - val_accuracy: 0.0950\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fba106c6358>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(patience=2, monitor='val_loss'),\n",
    "    tf.keras.callbacks.TensorBoard(log_dir='/home/yzk/logs')\n",
    "]\n",
    "model.fit(train_x, train_y, batch_size=16, epochs=5,\n",
    "          callbacks=callbacks, validation_data=(val_x, val_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多输入多输出模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset tf_flowers (218.21 MiB) to /home/yzk/tensorflow_datasets/tf_flowers/1.0.0...\u001b[0m\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "IntProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/.conda/envs/py36/lib/python3.6/site-packages/tqdm/notebook.py\u001b[0m in \u001b[0;36mstatus_printer\u001b[0;34m(_, total, desc, ncols)\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# No total? Show info style bar with no progress tqdm status\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m                 \u001b[0mpbar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mIntProgress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m                 \u001b[0mpbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'IntProgress' is not defined",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-4ef3a9ef9e0e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tf_flowers\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtfds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSplit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_supervised\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m255.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapplications\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMobileNetV2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py36/lib/python3.6/site-packages/tensorflow_datasets/core/api_utils.py\u001b[0m in \u001b[0;36mdisallow_positional_args_dec\u001b[0;34m(fn, instance, args, kwargs)\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0m_check_no_positional\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mismethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallowed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallowed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0m_check_required\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mdisallow_positional_args_dec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=no-value-for-parameter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py36/lib/python3.6/site-packages/tensorflow_datasets/core/registered.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, split, data_dir, batch_size, in_memory, shuffle_files, download, as_supervised, decoders, with_info, builder_kwargs, download_and_prepare_kwargs, as_dataset_kwargs, try_gcs)\u001b[0m\n\u001b[1;32m    298\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[0mdownload_and_prepare_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdownload_and_prepare_kwargs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m     \u001b[0mdbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload_and_prepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mdownload_and_prepare_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mas_dataset_kwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py36/lib/python3.6/site-packages/tensorflow_datasets/core/api_utils.py\u001b[0m in \u001b[0;36mdisallow_positional_args_dec\u001b[0;34m(fn, instance, args, kwargs)\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0m_check_no_positional\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mismethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallowed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallowed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0m_check_required\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mdisallow_positional_args_dec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=no-value-for-parameter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py36/lib/python3.6/site-packages/tensorflow_datasets/core/dataset_builder.py\u001b[0m in \u001b[0;36mdownload_and_prepare\u001b[0;34m(self, download_dir, download_config)\u001b[0m\n\u001b[1;32m    285\u001b[0m         self._download_and_prepare(\n\u001b[1;32m    286\u001b[0m             \u001b[0mdl_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdl_manager\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m             download_config=download_config)\n\u001b[0m\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m         \u001b[0;31m# NOTE: If modifying the lines below to put additional information in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py36/lib/python3.6/site-packages/tensorflow_datasets/core/dataset_builder.py\u001b[0m in \u001b[0;36m_download_and_prepare\u001b[0;34m(self, dl_manager, download_config)\u001b[0m\n\u001b[1;32m    946\u001b[0m     super(GeneratorBasedBuilder, self)._download_and_prepare(\n\u001b[1;32m    947\u001b[0m         \u001b[0mdl_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdl_manager\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mmax_examples_per_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdownload_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_examples_per_split\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m     )\n\u001b[1;32m    950\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py36/lib/python3.6/site-packages/tensorflow_datasets/core/dataset_builder.py\u001b[0m in \u001b[0;36m_download_and_prepare\u001b[0;34m(self, dl_manager, **prepare_split_kwargs)\u001b[0m\n\u001b[1;32m    802\u001b[0m     \u001b[0;31m# Generating data for all splits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    803\u001b[0m     \u001b[0msplit_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplits_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSplitDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 804\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0msplit_generator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_split_generators\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdl_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    805\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0msplits_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSplit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mALL\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0msplit_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m         raise ValueError(\n",
      "\u001b[0;32m~/.conda/envs/py36/lib/python3.6/site-packages/tensorflow_datasets/image/flowers.py\u001b[0m in \u001b[0;36m_split_generators\u001b[0;34m(self, dl_manager)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_split_generators\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdl_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdl_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload_and_extract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_URL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;31m# There is no predefined train/val/test split for this dataset.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py36/lib/python3.6/site-packages/tensorflow_datasets/core/download/download_manager.py\u001b[0m in \u001b[0;36mdownload_and_extract\u001b[0;34m(self, url_or_urls)\u001b[0m\n\u001b[1;32m    355\u001b[0m     \"\"\"\n\u001b[1;32m    356\u001b[0m     \u001b[0;31m# Add progress bar to follow the download state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_downloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extractor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_map_promise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_download_extract\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl_or_urls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py36/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't yield\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py36/lib/python3.6/site-packages/tensorflow_datasets/core/download/downloader.py\u001b[0m in \u001b[0;36mtqdm\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;34m\"\"\"Add a progression bar for the current download.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0masync_tqdm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_tqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0masync_tqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Dl Completed...'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m' url'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpbar_url\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0masync_tqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Dl Size...'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m' MiB'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpbar_dl_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pbar_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpbar_url\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py36/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't yield\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py36/lib/python3.6/site-packages/tensorflow_datasets/core/utils/tqdm_utils.py\u001b[0m in \u001b[0;36m_async_tqdm\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0mpbar\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAsync\u001b[0m \u001b[0mpbar\u001b[0m \u001b[0mwhich\u001b[0m \u001b[0mcan\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mshared\u001b[0m \u001b[0mbetween\u001b[0m \u001b[0mthreads\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m   \"\"\"\n\u001b[0;32m---> 98\u001b[0;31m   \u001b[0;32mwith\u001b[0m \u001b[0mtqdm_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpbar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m     \u001b[0mpbar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_TqdmPbarAsync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpbar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32myield\u001b[0m \u001b[0mpbar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py36/lib/python3.6/site-packages/tqdm/notebook.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0munit_scale\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m         self.container = self.status_printer(\n\u001b[0;32m--> 213\u001b[0;31m             self.fp, total, self.desc, self.ncols)\n\u001b[0m\u001b[1;32m    214\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py36/lib/python3.6/site-packages/tqdm/notebook.py\u001b[0m in \u001b[0;36mstatus_printer\u001b[0;34m(_, total, desc, ncols)\u001b[0m\n\u001b[1;32m    109\u001b[0m             \u001b[0;31m# #187 #451 #558\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m             raise ImportError(\n\u001b[0;32m--> 111\u001b[0;31m                 \u001b[0;34m\"IntProgress not found. Please update jupyter and ipywidgets.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m                 \u001b[0;34m\" See https://ipywidgets.readthedocs.io/en/stable\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m                 \"/user_install.html\")\n",
      "\u001b[0;31mImportError\u001b[0m: IntProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "num_batches = 1000\n",
    "batch_size = 50\n",
    "learning_rate = 0.001\n",
    "\n",
    "dataset = tfds.load(\"tf_flowers\", split=tfds.Split.TRAIN, as_supervised=True)\n",
    "dataset = dataset.map(lambda img, label: (tf.image.resize(img, [224, 224]) / 255.0, label)).shuffle(1024).batch(32)\n",
    "model = tf.keras.applications.MobileNetV2(weights=None, classes=5)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "for images, labels in dataset:\n",
    "    with tf.GradientTape() as tape:\n",
    "        labels_pred = model(images)\n",
    "        loss = tf.keras.losses.sparse_categorical_crossentropy(y_true=labels, y_pred=labels_pred)\n",
    "        loss = tf.reduce_mean(loss)\n",
    "        print(\"loss %f\" % loss.numpy())\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(grads_and_vars=zip(grads, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.dense1 = tf.keras.layers.Dense(units=24, activation=tf.nn.relu)\n",
    "        self.dense2 = tf.keras.layers.Dense(units=24, activation=tf.nn.relu)\n",
    "        self.dense3 = tf.keras.layers.Dense(units=2)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.dense1(inputs)\n",
    "        x = self.dense2(x)\n",
    "        x = self.dense3(x)\n",
    "        return x\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        q_values = self(inputs)\n",
    "        return tf.argmax(q_values, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/yzk/dist/BaseLib'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1014 09:31:48.207993 140389822482176 base_layer.py:1814] Layer q_network_18 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 3, epsilon 0.010000, score 46\n",
      "episode 4, epsilon 0.010000, score 41\n",
      "episode 7, epsilon 0.010000, score 45\n",
      "episode 11, epsilon 0.010000, score 17\n",
      "episode 12, epsilon 0.010000, score 15\n",
      "episode 13, epsilon 0.010000, score 40\n",
      "episode 14, epsilon 0.010000, score 35\n",
      "episode 18, epsilon 0.010000, score 43\n",
      "episode 19, epsilon 0.010000, score 44\n",
      "episode 20, epsilon 0.010000, score 37\n",
      "episode 22, epsilon 0.010000, score 34\n",
      "episode 23, epsilon 0.010000, score 37\n",
      "episode 24, epsilon 0.010000, score 49\n",
      "episode 25, epsilon 0.010000, score 36\n",
      "episode 26, epsilon 0.010000, score 38\n",
      "episode 27, epsilon 0.010000, score 46\n",
      "episode 31, epsilon 0.010000, score 48\n",
      "episode 32, epsilon 0.010000, score 36\n",
      "episode 33, epsilon 0.010000, score 31\n",
      "episode 34, epsilon 0.010000, score 44\n",
      "episode 35, epsilon 0.010000, score 49\n",
      "episode 39, epsilon 0.010000, score 48\n",
      "episode 40, epsilon 0.010000, score 45\n",
      "episode 41, epsilon 0.010000, score 46\n",
      "episode 42, epsilon 0.010000, score 32\n",
      "episode 43, epsilon 0.010000, score 46\n",
      "episode 44, epsilon 0.010000, score 47\n",
      "episode 46, epsilon 0.010000, score 47\n",
      "episode 48, epsilon 0.010000, score 33\n",
      "episode 49, epsilon 0.010000, score 38\n",
      "episode 51, epsilon 0.010000, score 33\n",
      "episode 53, epsilon 0.010000, score 28\n",
      "episode 57, epsilon 0.010000, score 23\n",
      "episode 58, epsilon 0.010000, score 33\n",
      "episode 59, epsilon 0.010000, score 35\n",
      "episode 60, epsilon 0.010000, score 34\n",
      "episode 61, epsilon 0.010000, score 26\n",
      "episode 62, epsilon 0.010000, score 44\n",
      "episode 63, epsilon 0.010000, score 29\n",
      "episode 67, epsilon 0.010000, score 35\n",
      "episode 69, epsilon 0.010000, score 43\n",
      "episode 70, epsilon 0.010000, score 39\n",
      "episode 71, epsilon 0.010000, score 40\n",
      "episode 72, epsilon 0.010000, score 33\n",
      "episode 73, epsilon 0.010000, score 27\n",
      "episode 74, epsilon 0.010000, score 49\n",
      "episode 76, epsilon 0.010000, score 25\n",
      "episode 77, epsilon 0.010000, score 48\n",
      "episode 83, epsilon 0.010000, score 47\n",
      "episode 85, epsilon 0.010000, score 38\n",
      "episode 91, epsilon 0.010000, score 42\n",
      "episode 93, epsilon 0.010000, score 25\n",
      "episode 94, epsilon 0.010000, score 29\n",
      "episode 99, epsilon 0.010000, score 35\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from collections import deque\n",
    "import random\n",
    "import numpy as np\n",
    "env = gym.make('CartPole-v1')       # 实例化一个游戏环境，参数为游戏名称\n",
    "model = QNetwork()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "replay_buffer = deque(maxlen=10000) # 使用一个 deque 作为 Q Learning 的经验回放池\n",
    "initial_epsilon= 0.001\n",
    "final_epsilon = 0.01\n",
    "epsilon = initial_epsilon\n",
    "num_episodes = 100\n",
    "num_exploration_episodes = 10\n",
    "max_len_episode = 50\n",
    "gamma = 0.01\n",
    "tf.keras.backend.set_floatx('float64')\n",
    "summary_writer = tf.summary.create_file_writer('./tensorboard') \n",
    "for episode_id in range(num_episodes):\n",
    "    state = env.reset()             # 初始化环境，获得初始状态\n",
    "    epsilon = max(                  # 计算当前探索率\n",
    "        initial_epsilon * (num_exploration_episodes - episode_id) / num_exploration_episodes,\n",
    "        final_epsilon)\n",
    "    for t in range(max_len_episode):\n",
    "#         env.render()                                # 对当前帧进行渲染，绘图到屏幕\n",
    "        if random.random() < epsilon:               # epsilon-greedy 探索策略，以 epsilon 的概率选择随机动作\n",
    "            action = env.action_space.sample()      # 选择随机动作（探索）\n",
    "        else:\n",
    "            action = model.predict(np.expand_dims(state, axis=0)).numpy()   # 选择模型计算出的 Q Value 最大的动作\n",
    "            action = action[0]\n",
    "\n",
    "        # 让环境执行动作，获得执行完动作的下一个状态，动作的奖励，游戏是否已结束以及额外信息\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        # 如果游戏Game Over，给予大的负奖励\n",
    "        reward = -10. if done else reward\n",
    "        # 将(state, action, reward, next_state)的四元组（外加 done 标签表示是否结束）放入经验回放池\n",
    "        replay_buffer.append((state, action, reward, next_state, 1 if done else 0))\n",
    "        # 更新当前 state\n",
    "        state = next_state\n",
    "\n",
    "        if done:                                    # 游戏结束则退出本轮循环，进行下一个 episode\n",
    "            print(\"episode %d, epsilon %f, score %d\" % (episode_id, epsilon, t))\n",
    "            break\n",
    "\n",
    "        if len(replay_buffer) >= batch_size:\n",
    "            # 从经验回放池中随机取一个批次的四元组，并分别转换为 NumPy 数组\n",
    "            batch_state, batch_action, batch_reward, batch_next_state, batch_done = zip(\n",
    "                *random.sample(replay_buffer, batch_size))\n",
    "            batch_state, batch_reward, batch_next_state, batch_done = \\\n",
    "                [np.array(a, dtype=np.float32) for a in [batch_state, batch_reward, batch_next_state, batch_done]]\n",
    "            batch_action = np.array(batch_action, dtype=np.int32)\n",
    "            \n",
    "            q_value = model(batch_next_state)\n",
    "            y = batch_reward + (gamma * tf.reduce_max(q_value, axis=1)) * (1 - batch_done)  # 计算 y 值\n",
    "            with tf.GradientTape() as tape:\n",
    "                loss = tf.keras.losses.mean_squared_error(  # 最小化 y 和 Q-value 的距离\n",
    "                    y_true=y,\n",
    "                    y_pred=tf.reduce_sum(model(batch_state) * tf.one_hot(batch_action, depth=2), axis=1)\n",
    "                )\n",
    "            \n",
    "            with summary_writer.as_default():\n",
    "                tf.summary.scalar('loss', loss, step=episode_id)\n",
    "                tf.summary.scalar('reward', batch_reward.sum(), step=episode_id)\n",
    "            grads = tape.gradient(loss, model.variables)\n",
    "            optimizer.apply_gradients(grads_and_vars=zip(grads, model.variables))       # 计算梯度并"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tensorflow_probability as tfp\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAME = 'Pendulum-v0'\n",
    "env = gym.make(GAME)\n",
    "N_A = env.action_space.shape[0]\n",
    "N_S = env.observation_space.shape[0]\n",
    "A_BOUND = [env.action_space.low, env.action_space.high]\n",
    "ENTROPY_BETA = 0.01\n",
    "\n",
    "class ACNet(tf.keras.Model):\n",
    "    def __init__(self, globalAC=None):\n",
    "        super().__init__()\n",
    "        self.globalAC = globalAC\n",
    "        self.la = tf.keras.layers.Dense(units=200, activation=tf.nn.relu)\n",
    "        self.mu = tf.keras.layers.Dense(units=N_A, activation=tf.nn.tanh)  \n",
    "        self.sigma = tf.keras.layers.Dense(units=N_A, activation=tf.nn.softplus)\n",
    "        self.lc = tf.keras.layers.Dense(100, activation=tf.nn.relu)\n",
    "        self.v = tf.keras.layers.Dense(N_A, activation=tf.nn.softmax)\n",
    "            \n",
    "    def call(self, state):\n",
    "        x = self.la(state)\n",
    "        mu = self.mu(x)\n",
    "        sigma = self.sigma(x)\n",
    "        v_s = self.lc(state)\n",
    "        self.v_s = self.v(v_s)\n",
    "        tf.multiply(mu, A_BOUND[1])\n",
    "        sigma = sigma + 1e-4\n",
    "        self.norm_dist = tfp.distributions.Normal(mu, sigma)\n",
    "        return mu, sigma, self.v_s\n",
    "    \n",
    "    def choose_action(self, state):\n",
    "        self(state)\n",
    "        return tf.clip_by_value(self.norm_dist.sample(1), A_BOUND[0], A_BOUND[1])\n",
    "    \n",
    "    def update_global(self, state, a_his, v_target):\n",
    "        mu, sigma, v_s = self(state)\n",
    "        td = tf.subtract(v_target, v_s)\n",
    "        \n",
    "        entropy = self.norm_dist.entropy()\n",
    "        a_prob = self.norm_dist.log_prob(a_his)\n",
    "        \n",
    "        exp_v = a_prob * tf.stop_gradient(td) + entropy * ENTROPY_BETA\n",
    "        \n",
    "#         update global net's paramerter\n",
    "        with tf.GradientTape() as tape:\n",
    "            c_loss = tf.reduce_mean(tf.square(td))\n",
    "        grad_c = tape.gradient(c_loss, self.variables)\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            a_loss = tf.reduce_mean(-exp_v)\n",
    "        grad_a = tape.gradient(a_loss, self.variables)\n",
    "        \n",
    "        OPT_C.apply_gradients(grads_and_vars=zip(grad_c, self.globalAC.variables))\n",
    "        OPT_A.apply_gradients(grads_and_vars=zip(grad_a, self.globalAC.variables))\n",
    "        \n",
    "    def pull_global(self):\n",
    "        for l_param, g_param in zip(self.variables, self.globalAC.variables):\n",
    "            l_param.assign(g_param)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = ACNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.4110373 , 0.91161853, 0.05072076])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = env.reset()\n",
    "a = env.action_space.sample()\n",
    "s_, r, done, info = env.step(a)\n",
    "s_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1014 20:23:18.809545 139628629841664 base_layer.py:1814] Layer ac_net is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=177, shape=(1, 1), dtype=float32, numpy=array([[1.0364757]], dtype=float32)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = net.choose_action(s_.reshape((1, -1))).numpy().ravel()\n",
    "net.norm_dist.entropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_GLOBAL_EP = 200\n",
    "MAX_EPISODE = 200\n",
    "UPDATE_GLOBAL_ITER = 10\n",
    "GLOABL_REWARD_R = []\n",
    "GLOBAL_EP = 0\n",
    "GAMMA = 0.9\n",
    "        \n",
    "class Worker():\n",
    "    def __init__(self,name, globalAC):\n",
    "        self.env = gym.make(GAME).unwrapped\n",
    "        self.name = name \n",
    "        self.ACNet = ACNet(globalAC)\n",
    "    def work(self):\n",
    "        total_step = 1\n",
    "        buffer_s, buffer_a, buffer_r = [], [], []\n",
    "        global GLOBAL_EP, GLOABL_REWARD_R\n",
    "        while not COORD.should_stop() and GLOBAL_EP < MAX_GLOBAL_EP:\n",
    "            ep_r = 0\n",
    "            s = self.env.reset()\n",
    "            for ep_t in range(MAX_EPISODE):\n",
    "                a = self.ACNet.choose_action(s.reshape(1, -1)).numpy().ravel()\n",
    "                s_ , r, done, info = self.env.step(a)\n",
    "                ep_r += r\n",
    "                done = True if ep_t == MAX_EPISODE -1 else False\n",
    "                buffer_s.append(s)\n",
    "                buffer_a.append(a)\n",
    "                buffer_r.append((r +  8)/ 8)\n",
    "                if total_step % UPDATE_GLOBAL_ITER ==0 or done:\n",
    "                    if done:\n",
    "                        v_s_ = 0\n",
    "                    else:\n",
    "                        v_s_ = self.ACNet(s_.reshape(1, -1))[-1].numpy().ravel()\n",
    "                    buffer_v_target = []\n",
    "                    for item in buffer_r[::-1]:\n",
    "                        v_s_  = v_s_ * GAMMA + item\n",
    "                        buffer_v_target.append(v_s_)\n",
    "                    buffer_v_target.reverse()\n",
    "                    buffer_s, buffer_a, buffer_v_target = np.vstack(buffer_s), np.vstack(buffer_a), np.vstack(buffer_v_target)\n",
    "                    self.ACNet.update_global(buffer_s, buffer_a, buffer_v_target)\n",
    "                    buffer_a,buffer_s,buffer_r = [],[],[]\n",
    "                    self.ACNet.pull_global()\n",
    "\n",
    "                total_step += 1\n",
    "                s = s_\n",
    "                if done:\n",
    "                    if len(GLOABL_REWARD_R) == 0:\n",
    "                        GLOABL_REWARD_R.append(ep_r)\n",
    "                    else:\n",
    "                        GLOABL_REWARD_R.append(GLOABL_REWARD_R[-1]*0.9 + 0.1*ep_r)\n",
    "                    print(self.name, \"EP:\",GLOBAL_EP, \"Reward:\", GLOABL_REWARD_R[-1])\n",
    "                    GLOBAL_EP +=1    \n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1014 20:23:34.572710 139610277517056 base_layer.py:1814] Layer ac_net_2 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "W1014 20:23:34.575170 139609791002368 base_layer.py:1814] Layer ac_net_3 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "W1014 20:23:34.576746 139609782609664 base_layer.py:1814] Layer ac_net_4 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "W1014 20:23:34.578409 139609774216960 base_layer.py:1814] Layer ac_net_5 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "W1014 20:23:34.579856 139609765824256 base_layer.py:1814] Layer ac_net_6 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "W1014 20:23:34.581335 139609757431552 base_layer.py:1814] Layer ac_net_7 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "W1014 20:23:34.582772 139609749038848 base_layer.py:1814] Layer ac_net_8 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "W1014 20:23:34.584194 139609740646144 base_layer.py:1814] Layer ac_net_9 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "W1014 20:23:34.586224 139609187022592 base_layer.py:1814] Layer ac_net_10 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "W1014 20:23:34.587900 139609178629888 base_layer.py:1814] Layer ac_net_11 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "W1014 20:23:34.589348 139609170237184 base_layer.py:1814] Layer ac_net_12 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1014 20:23:34.590804 139609161844480 base_layer.py:1814] Layer ac_net_13 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "W1014 20:23:34.592258 139609153451776 base_layer.py:1814] Layer ac_net_14 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "W1014 20:23:34.593709 139609145059072 base_layer.py:1814] Layer ac_net_15 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "W1014 20:23:34.595308 139609136666368 base_layer.py:1814] Layer ac_net_16 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "W1014 20:23:34.597384 139608650151680 base_layer.py:1814] Layer ac_net_17 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "W1014 20:23:34.598870 139608641758976 base_layer.py:1814] Layer ac_net_18 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "W1014 20:23:34.600508 139608633366272 base_layer.py:1814] Layer ac_net_19 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "W1014 20:23:34.601918 139608624973568 base_layer.py:1814] Layer ac_net_20 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "W1014 20:23:34.603542 139608616580864 base_layer.py:1814] Layer ac_net_21 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "W1014 20:23:34.605072 139608608188160 base_layer.py:1814] Layer ac_net_22 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "W1014 20:23:34.606521 139608599795456 base_layer.py:1814] Layer ac_net_23 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1014 20:23:34.608612 139608113280768 base_layer.py:1814] Layer ac_net_24 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "W1014 20:23:34.610073 139608104888064 base_layer.py:1814] Layer ac_net_25 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "W1014 20:23:34.611556 139608096495360 base_layer.py:1814] Layer ac_net_26 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "W1014 20:23:34.613124 139608088102656 base_layer.py:1814] Layer ac_net_27 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "W1014 20:23:34.614538 139608079709952 base_layer.py:1814] Layer ac_net_28 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "W1014 20:23:34.616630 139608071317248 base_layer.py:1814] Layer ac_net_29 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "W1014 20:23:34.618408 139608062924544 base_layer.py:1814] Layer ac_net_30 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "W1014 20:23:34.620809 139607576409856 base_layer.py:1814] Layer ac_net_31 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "W1014 20:23:34.623773 139607568017152 base_layer.py:1814] Layer ac_net_32 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "W1014 20:23:34.628490 139607559624448 base_layer.py:1814] Layer ac_net_33 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "W1014 20:23:34.630493 139607551231744 base_layer.py:1814] Layer ac_net_34 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1014 20:23:34.632501 139607542839040 base_layer.py:1814] Layer ac_net_35 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "W1014 20:23:34.634467 139607534446336 base_layer.py:1814] Layer ac_net_36 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "W1014 20:23:34.638715 139607526053632 base_layer.py:1814] Layer ac_net_37 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "W1014 20:23:34.644406 139607039538944 base_layer.py:1814] Layer ac_net_38 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "W1014 20:23:34.646503 139607031146240 base_layer.py:1814] Layer ac_net_39 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "W1014 20:23:34.651270 139607022753536 base_layer.py:1814] Layer ac_net_40 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "W1014 20:23:34.657665 139607014360832 base_layer.py:1814] Layer ac_net_41 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "W1014 20:23:34.664823 139607005968128 base_layer.py:1814] Layer ac_net_42 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "W1014 20:23:34.673349 139606997575424 base_layer.py:1814] Layer ac_net_43 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "W1014 20:23:34.677345 139606989182720 base_layer.py:1814] Layer ac_net_44 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "W1014 20:23:34.683038 139606502668032 base_layer.py:1814] Layer ac_net_45 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1014 20:23:34.685012 139606494275328 base_layer.py:1814] Layer ac_net_46 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "W1014 20:23:34.688280 139606485882624 base_layer.py:1814] Layer ac_net_47 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "W1014 20:23:34.691816 139606477489920 base_layer.py:1814] Layer ac_net_48 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "W1014 20:23:34.697261 139606469097216 base_layer.py:1814] Layer ac_net_49 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "W1014 20:23:34.704087 139606460704512 base_layer.py:1814] Layer ac_net_50 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "W1014 20:23:34.714015 139606452311808 base_layer.py:1814] Layer ac_net_51 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "W1014 20:23:34.723712 139605965797120 base_layer.py:1814] Layer ac_net_52 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "W1014 20:23:34.727943 139605957404416 base_layer.py:1814] Layer ac_net_53 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "W1014 20:23:34.732081 139605949011712 base_layer.py:1814] Layer ac_net_54 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "W1014 20:23:34.734449 139605940619008 base_layer.py:1814] Layer ac_net_55 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "W1014 20:23:34.736892 139605932226304 base_layer.py:1814] Layer ac_net_56 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1014 20:23:34.742203 139605923833600 base_layer.py:1814] Layer ac_net_57 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_8 EP: 0 Reward: -1070.5248722727922\n",
      "W_2 EP: 1W_44 EP: 1  Reward: -1069.6922748776738\n",
      "Reward: -1065.6890099051598\n",
      "W_1 EP: 3 Reward: -1069.1664554789222\n",
      "W_6 EP: 4W_13 EP: 4 Reward:  Reward: -1068.8505098557632\n",
      "-1075.6154390938977\n",
      "W_42 EP: 6 Reward: -1087.9226660185032\n",
      "W_9 EP: 7 Reward: -1141.2496030068855\n",
      "W_39W_19 EP: 8  EP: 8 Reward: Reward: -1147.5942089706004\n",
      "W_12 -1161.3452233087821\n",
      "EP: 9 Reward: -1154.5115704761042\n",
      "W_11 EP: 11 Reward: -1160.743237853619\n",
      "W_18 EP: 12 Reward: -1174.6443783632574\n",
      "W_22 EP: 13 Reward: -1208.6890263723794\n",
      "W_17 EP: 14 Reward: -1167.7884330924521\n",
      "W_7 EP: 15 Reward:W_31 EP: 15 -1156.800569707175\n",
      " Reward: -1227.086692495218\n",
      "W_37 EP: 17 Reward: -1213.3848005067937\n",
      "W_35 EP: 18 Reward: -1255.0107486392485\n",
      "W_30 EP: 18 Reward:W_21 EP: 19 Reward: -1308.6744966263211\n",
      " -1292.2283399367968\n",
      "W_25 EP: 21 Reward: -1308.6706271880396\n",
      "W_26 EP: 22 Reward: -1314.5538223430262\n",
      "W_3 EP: W_20 EP: 23 Reward: -1338.8944314165715\n",
      "23 Reward: -1349.7309136521167\n",
      "W_0 EP: 25 Reward: -1320.7603134914177\n",
      "W_34 EP: 26 Reward: -1356.1529221783512\n",
      "W_14 EP: 27W_36 Reward: EP: 27 Reward: -1426.4838437191452\n",
      " -1386.444997425853\n",
      "W_54 EP: 29 Reward: -1414.707279596799\n",
      "W_45 EP: 30 Reward: -1369.4350319282073\n",
      "W_24 EP: 31 Reward: -1380.265518392359\n",
      "W_50 EP: 32 Reward: -1331.6343445762777\n",
      "W_32 EP: 33 Reward: -1319.8045647870879\n",
      "W_5 EP: 34 Reward:W_15 EP: 34  Reward: -1294.887553760858-1323.1322329449267\n",
      "\n",
      "W_29 EP: 36 Reward: -1290.9548954592658\n",
      "W_33 EP: 37 Reward: -1311.7766063970817\n",
      "W_23 EP: 38 Reward: -1265.2863649582746\n",
      "W_53 EP: 39 W_28 Reward: -1254.8270348976346\n",
      "W_27 EP: EP: 39 Reward: -1289.1159970633162\n",
      "40 Reward: -1271.1862055291103\n",
      "W_46 EP: 42 Reward: -1275.7883315863862\n",
      "W_4 EP: W_1643 Reward: -1322.4977080129115\n",
      " EP: 43 Reward: -1366.879006927693\n",
      "W_10 EP: W_49 EP: 45 Reward: -1349.2517277881814\n",
      "45 Reward: -1365.1923807972007\n",
      "W_47 EP: 47 Reward: -1347.5445553958825\n",
      "W_40 EP: 48 Reward: -1309.6914773351577\n",
      "W_38 EP: 49 Reward: -1362.1473841499098\n",
      "W_51 EP: 50 Reward: -1379.942378719455\n",
      "W_48 EP: 51 Reward: -1338.5686251784648\n",
      "W_41 EP: 52 Reward: -1320.5512299384022\n",
      "W_55 EP: 53 Reward: -1293.625434104578\n",
      "W_43 EP: 54 Reward: -1282.250011338331\n",
      "W_52 EP: 55 Reward: -1288.398989902594\n",
      "W_18 EP: 56 Reward: -1329.796085873387\n",
      "W_17 EP: 57 Reward: -1285.471008304448\n",
      "W_37 EP: 58 Reward: -1269.6949071884605\n",
      "W_1 EP: 59 W_7Reward: EP: 59 Reward: -1315.5352894871608\n",
      " -1318.198325462616\n",
      "W_21 EP: 61 Reward: -1291.588905812761\n",
      "W_55 EP: 62 Reward: -1292.4329344600865\n",
      "W_6 EP: W_1263 Reward: -1273.7268693532085\n",
      "W_8 EP: 64 Reward: -1228.9504329858155\n",
      " W_26 EP: 65 Reward: -1237.2379263620192\n",
      "EP: 63 Reward: -1235.3456224653814\n",
      "W_9 EP: 67 Reward: -1241.9677192116112\n",
      "W_20 EP: 68 Reward: -1237.941568072863W_2 EP: 68\n",
      " Reward: -1246.0531042353257\n",
      "W_44 EP: 70 Reward: -1231.195260994253W_22 EP: 70 Reward: \n",
      "W_34 EP: 71 Reward: -1303.595754844707-1269.758812104038\n",
      "\n",
      "W_30 EP: 73 Reward: -1314.695934960791\n",
      "W_3 EP: 74 Reward: -1342.4253153226666W_13\n",
      " EP: 74 Reward: -1326.4451394363066\n",
      "W_35 EP: 76 Reward: -1303.6348467543771\n",
      "W_45 EP: 77 Reward: -1260.6953762841852\n",
      "W_11 EP: 78 Reward: -1313.8542609444826\n",
      "W_0 EP: 79 Reward: -1348.1355086502217\n",
      "W_25 EP: 80 Reward: -1347.3286012157087\n",
      "W_31 EP: 81 Reward: -1318.2433610955698\n",
      "W_15 EP: 82 Reward: -1317.7944201340372\n",
      "W_24 EP: 83 Reward: -1333.300118303027\n",
      "W_50 EP: 84 Reward: -1345.0841871483522\n",
      "W_23 EP: 85 Reward: -1307.6822233909145\n",
      "W_42 EP: 86 Reward: -1309.528089650371\n",
      "W_36 EP: 87W_19 EP: 87  Reward: -1296.7052547258447\n",
      "W_51 EP: 88 Reward: Reward: -1284.6234987702203\n",
      "-1311.7615269134617\n",
      "W_54 EP: 90 Reward: -1288.7761884505298\n",
      "W_41 EP: 91 Reward: -1328.0054439823334\n",
      "W_14 EP: 92 Reward: -1352.415642268369\n",
      "W_32 EP: 93W_52 Reward:  EP: 93 Reward: -1357.0448413167856\n",
      "-1333.512487015715\n",
      "W_5 EP: 95 Reward: -1387.0688452324036\n",
      "W_28 EP: 96 Reward: -1415.073469661711\n",
      "W_39 EP: 97 Reward: -1417.8073403061492\n",
      "W_29 EP: 98 Reward: -1391.5130249319595\n",
      "W_47 EP: 99 Reward: -1371.0248347566358\n",
      "W_33 W_10 EP: 100 Reward: -1441.338844910835EP: 100 Reward: \n",
      "-1402.610805784091\n",
      "W_49 EP: 102 Reward: -1401.9620780446453\n",
      "W_53 EP: 103 Reward: -1407.1343091949868\n",
      "W_16 EP: 104 Reward: -1358.2922106556498\n",
      "W_46 EP: 105 Reward: -1398.6338420958614\n",
      "W_4W_43 EP: 106 Reward: -1322.8402116860664\n",
      " EP: 106 Reward: -1356.300113441651\n",
      "W_27 EP: 108 Reward: -1313.3676241717249\n",
      "W_38 EP: 109 Reward: -1312.7381437696888\n",
      "W_40 EP: 110 Reward: -1335.5531019429043\n",
      "W_48 EP: 111 Reward: -1359.5238717171035\n",
      "W_18 EP: 112 Reward: -1330.6665685673365\n",
      "W_35W_8  EP: 113 EP: 113 Reward: -1341.2829130023122\n",
      "Reward: -1339.2536389040158\n",
      "W_7 EP: 115 Reward: -1320.7843159871416\n",
      "W_22 EP: 116 Reward: -1333.2412576531176\n",
      "W_37 EP: 117 Reward: -1353.4953742602845\n",
      "W_55 EP: 118 Reward: -1307.6638634394687\n",
      "W_1 EP: 119 Reward: -1280.3217043925408\n",
      "W_2 EP: 120 Reward: -1305.8524262627452\n",
      "W_13 EP: 121 Reward: -1281.955368138901\n",
      "W_44 EP: 122 Reward: -1258.4467242001754\n",
      "W_17 EP: 123 Reward: -1239.4794838447635\n",
      "W_52 EP: 124 Reward: -1236.7604285670686\n",
      "W_21 EP: 125 Reward: -1229.2883375412616\n",
      "W_30 EP: 126 Reward: -1231.135897454528\n",
      "W_20 EP: 127 Reward: -1288.9481392579855\n",
      "W_9 EP: 128 Reward: -1257.2340022154567\n",
      "W_6W_25 EP: 129 Reward: EP: 129 Reward: -1269.2246469180016\n",
      " -1314.1781996015845\n",
      "W_34W_31 EP: W_26131 Reward: -1285.121583234772\n",
      " EP: 131 Reward: -1320.8169552116367\n",
      " EP: 131 Reward: -1301.5652707046256\n",
      "W_45 EP: 134 Reward:W_53 EP: 134 Reward: -1354.9990451693843\n",
      " -1352.1633782216213\n",
      "W_3 EP: 136 Reward: -1336.801952586194\n",
      "W_11 EP: 137 Reward: -1331.9705135896356\n",
      "W_54 EP: 138 Reward: -1347.53162970521\n",
      "W_24 EP: 139 Reward: -1341.3280370246182\n",
      "W_32 EP: 140 Reward: -1339.9401271766515\n",
      "W_5 EP: 141 Reward: -1360.7696578440316\n",
      "W_23 EP: 142 Reward: -1369.8713220946079\n",
      "W_36 EP: 143W_12 EP: 143 Reward: -1331.1130881301879\n",
      " Reward: -1348.7323217575856\n",
      "W_29 EP: 145 Reward: -1310.513777886699\n",
      "W_33 EP: 146 Reward: -1279.9253677689271\n",
      "W_15 EP: 147 Reward: W_42 EP: 147 Reward: -1335.5125460737559\n",
      "-1279.640541644992\n",
      "W_41 EP: 149 Reward: -1298.615850710893\n",
      "W_28 EP: 150 Reward: -1298.147608468789\n",
      "W_46 EP: 151 Reward: -1281.482772533241\n",
      "W_10 EP: 152 Reward: -1278.2105754209294\n",
      "W_14 EP: 153 Reward: -1337.987025497147\n",
      "W_16 EP: 154 Reward: -1328.8124660744518\n",
      "W_49 EP: 155 Reward: -1351.5662922246406\n",
      "W_51 EP: 156 Reward: -1400.2849288219747\n",
      "W_50 EP: 157 Reward: -1390.2771910131794\n",
      "W_27 EP: 158 Reward: -1422.2836587835804\n",
      "W_0W_38 EP: 159 Reward: -1459.9046753787636 EP: 159 Reward:\n",
      " W_19 EP: 159 Reward: -1406.4904553973913\n",
      "-1403.9543154974735\n",
      "W_4W_39 EP: 162 Reward: -1348.8954690692349\n",
      " EP: 162 Reward: -1365.13489867334\n",
      "W_47 EP: 164 Reward: -1349.9168020085247\n",
      "W_43 EP: 165 Reward: -1320.5047626588632\n",
      "W_48 EP: 166 Reward: -1329.950188279875\n",
      "W_40 EP: 167 Reward: -1325.9660086233544\n",
      "W_37 EP: 168 Reward: -1296.2193204184287\n",
      "W_35 EP: 169 Reward: -1267.2354017968773\n",
      "W_8 EP: 170 Reward: -1247.1066062323957\n",
      "W_17 EP: 171 Reward: -1250.9498808804403\n",
      "W_9W_18 EP: 172 Reward: -1235.986880495309\n",
      " EP: 172 Reward: -1222.424073229761\n",
      "W_21 EP: 174 Reward: -1232.9322182146818\n",
      "W_22 EP: 175 Reward: -1257.0597112321616\n",
      "W_52 EP: 176 Reward: -1259.9670062714022\n",
      "W_53 EP: 177 Reward: -1254.8034360593992\n",
      "W_13 EP: 178 Reward: -1271.090445271008\n",
      "W_6 EP: 179 Reward: -1263.9482658998381\n",
      "W_3 EP: 180 Reward: -1321.212994643351\n",
      "W_2 EP: 181 Reward: -1375.402637246918\n",
      "W_44 EP: 182 Reward: -1349.6811959930426\n",
      "W_55 EP: 183 Reward: -1351.4125240533074\n",
      "W_36 EP: 184 Reward: -1349.0924051625211\n",
      "W_7 EP: 185 Reward: -1327.8751995944322\n",
      "W_20 EP: 186 Reward: -1367.5121244784432\n",
      "W_31 EP: 187 Reward: -1326.4204732391356\n",
      "W_54 EP: 188 Reward: -1330.781067121673\n",
      "W_1 EP: 189 Reward:W_26 EP: 189 Reward:W_33 EP: 189 Reward: -1318.2823709959716\n",
      " -1356.5592287035377\n",
      " -1332.9113380286522\n",
      "W_30 EP: 192 Reward: -1314.7073368523136\n",
      "W_42 EP: 193W_25 EP: 193 Reward: -1353.2696455326086\n",
      " Reward: -1357.1158936916759\n",
      "W_23W_45 EP: 195 Reward: -1335.3988034842068\n",
      " EP: 195 Reward: -1356.488376037543\n",
      "W_41 EP: 197 Reward: -1321.053297126633\n",
      "W_32 EP: 198 Reward: -1296.3896476807627\n",
      "W_14 EP: 199 Reward: -1294.3466677064794\n",
      "W_38 EP: 200 Reward: -1302.9700058472708\n",
      "W_34 EP: 200 W_11Reward: -1316.4623814341837\n",
      " EP: 200 Reward: -1363.4234480198957\n",
      "W_0 EP: 203 Reward: -1400.7764577359228\n",
      "W_46 EP: 204 Reward: -1390.55001597811\n",
      "W_29 EP: 205 Reward: -1363.3084782657702\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_50 EP: 206 Reward: -1332.0920359566846\n",
      "W_12 EP: 207 Reward: -1298.794904548911\n",
      "W_51 EP: 208 Reward: -1323.0902252500757\n",
      "W_24 EP: 209 Reward: -1348.542054268582\n",
      "W_10 EP: 210 Reward: -1302.156124081564\n",
      "W_49 EP: 211 Reward: -1354.9218418907255\n",
      "W_16W_39 EP:  212 Reward: -1340.2124437370992\n",
      "EP: 212 Reward: -1324.3921743259004\n",
      "W_15 EP: 214 Reward: -1315.4545974588511\n",
      "W_5 EP: 215 Reward: -1297.5600100298323\n",
      "W_27 EP: 216 Reward: -1339.4859755856326\n",
      "W_28 EP: 217 Reward: -1390.5280829924397\n",
      "W_4 EP: 218 Reward: -1358.2744849015687\n",
      "W_19 EP: 219 Reward: -1339.8516698067808\n",
      "W_48 EP: 220 Reward: -1311.8790863146173\n",
      "W_43W_47 EP: 221 Reward: -1388.3118316170528 EP: 221 Reward:\n",
      " -1350.8946763031727\n",
      "W_40 EP: 223 Reward: -1384.4515301832398\n",
      "W_37 EP: 224 Reward: -1347.4771108350785\n",
      "W_8 EP: 225 Reward: -1360.854286375281\n",
      "W_35 EP: 226 Reward: -1366.200983901487\n",
      "W_17 EP: 227 Reward: -1348.0125302563172\n",
      "W_36 EP: 228 Reward: -1394.1270975549185\n",
      "W_9 EP: 229 Reward: -1353.5929884203392\n",
      "W_21 EP: 230 Reward: -1358.6939302363792\n",
      "W_44 EP: 231 Reward: -1390.409941817842\n",
      "W_26 EP: 232 Reward: -1369.4477668652144\n",
      "W_6 EP: 233 Reward: -1415.8414876279903\n",
      "W_7 EP: 234 Reward: -1384.8856843787992\n",
      "W_13 EP: 235 Reward:W_3 EP:  -1388.5774551466352\n",
      "235 Reward: -1356.808043362805\n",
      "W_22 EP: 237 Reward: -1345.1121219501524\n",
      "W_52 EP: 238W_31 EP: 238 Reward: -1343.1933626060018 W_53 EP: 238 Reward: -1310.0463042310917Reward:\n",
      "\n",
      " -1324.6024057476393\n",
      "W_18 EP: 241 Reward: -1284.9667611143764\n",
      "W_45 EP: 242 Reward:W_2 EP: 242 -1264.2605331640934\n",
      " Reward: -1257.6609484450314\n",
      "W_55 EP: 244 Reward: -1298.3049429869875\n",
      "W_23 EP: 245 Reward: -1278.3742574243333\n",
      "W_1 EP: 246 Reward: -1310.853687003516\n",
      "W_42 EP: 246 Reward: -1322.031396279538\n",
      "W_54 EP: 248 Reward: -1292.669500624011\n",
      "W_33 EP: 249 Reward: -1263.7745202341832\n",
      "W_41 EP: 250 Reward: -1283.7752546050615\n",
      "W_32 EP: 251 Reward: -1237.07559984051\n",
      "W_25 EP: 252 W_20 EP: 252 Reward:Reward:  -1256.9682213541716\n",
      "-1251.707208104353\n",
      "W_30 EP: 254 Reward: -1246.947958716938\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import multiprocessing\n",
    "import threading\n",
    "LR_A = 0.0001\n",
    "LR_C = 0.001\n",
    "N_WORKERS = multiprocessing.cpu_count()\n",
    "summary_writer = tf.summary.create_file_writer('./tensorboard') \n",
    "with tf.device(\"/cpu:0\"):\n",
    "    OPT_A = tf.keras.optimizers.RMSprop(LR_A)\n",
    "    OPT_C = tf.keras.optimizers.RMSprop(LR_C)\n",
    "    GLOBAL_AC = ACNet()  # we only need its params\n",
    "    workers = []\n",
    "    # Create worker\n",
    "    for i in range(N_WORKERS):\n",
    "        i_name = 'W_%i' % i   # worker name\n",
    "        workers.append(Worker(i_name, GLOBAL_AC))\n",
    "COORD = tf.train.Coordinator()\n",
    "\n",
    "worker_threads = []\n",
    "for worker in workers:\n",
    "    job = lambda: worker.work()\n",
    "    t = threading.Thread(target=job)\n",
    "    t.start()\n",
    "    worker_threads.append(t)\n",
    "COORD.join(worker_threads)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(np.arange(len(GLOABL_REWARD_R)), GLOABL_REWARD_R)\n",
    "plt.xlabel('step')\n",
    "plt.ylabel('Total moving reward')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36_tf2",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
