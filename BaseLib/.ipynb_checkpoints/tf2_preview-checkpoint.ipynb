{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n",
      "2.2.4-tf\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "print(tf.__version__)\n",
    "print(tf.keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## keras 堆叠模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(layers.Dense(32, activation='relu'))\n",
    "model.add(layers.Dense(32, activation='relu'))\n",
    "model.add(layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "              loss=tf.keras.losses.categorical_crossentropy,\n",
    "              metrics=[tf.keras.metrics.categorical_accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples, validate on 200 samples\n",
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 0s 475us/sample - loss: 11.9318 - categorical_accuracy: 0.1040 - val_loss: 12.0681 - val_categorical_accuracy: 0.1050\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 0s 30us/sample - loss: 12.4159 - categorical_accuracy: 0.1040 - val_loss: 12.9323 - val_categorical_accuracy: 0.0850\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 0s 29us/sample - loss: 13.6672 - categorical_accuracy: 0.0870 - val_loss: 14.7215 - val_categorical_accuracy: 0.0750\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 0s 29us/sample - loss: 15.9045 - categorical_accuracy: 0.0970 - val_loss: 17.6465 - val_categorical_accuracy: 0.1000\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 0s 29us/sample - loss: 19.3389 - categorical_accuracy: 0.1080 - val_loss: 21.6197 - val_categorical_accuracy: 0.1050\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 0s 29us/sample - loss: 23.4647 - categorical_accuracy: 0.1040 - val_loss: 25.8203 - val_categorical_accuracy: 0.0950\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 0s 29us/sample - loss: 28.2279 - categorical_accuracy: 0.1070 - val_loss: 31.5120 - val_categorical_accuracy: 0.0950\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 0s 27us/sample - loss: 34.7599 - categorical_accuracy: 0.1050 - val_loss: 38.9985 - val_categorical_accuracy: 0.0900\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 0s 28us/sample - loss: 42.6738 - categorical_accuracy: 0.0970 - val_loss: 47.4111 - val_categorical_accuracy: 0.0900\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 0s 29us/sample - loss: 51.7570 - categorical_accuracy: 0.0990 - val_loss: 56.8850 - val_categorical_accuracy: 0.0950\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fba90523278>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "train_x = np.random.random((1000, 72))\n",
    "train_y = np.random.random((1000, 10))\n",
    "val_x = np.random.random((200, 72))\n",
    "val_y = np.random.random((200, 10))\n",
    "model.fit(train_x, train_y, epochs=10, batch_size=100,\n",
    "          validation_data=(val_x, val_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 38us/sample - loss: 56.3595 - categorical_accuracy: 0.0890\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[56.35947882080078, 0.089]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x = np.random.random((1000, 72))\n",
    "test_y = np.random.random((1000, 10))\n",
    "\n",
    "model.evaluate(test_x, test_y, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.13609815e-06, 4.39963710e-09, 6.00056246e-13, ...,\n",
       "        2.10448071e-01, 1.08942135e-04, 3.01993615e-03],\n",
       "       [5.94892617e-06, 4.09629379e-08, 1.05548677e-11, ...,\n",
       "        2.24327385e-01, 1.72162268e-04, 6.30125776e-03],\n",
       "       [8.56924089e-06, 6.37576960e-08, 1.22256727e-11, ...,\n",
       "        1.98053017e-01, 2.16444823e-04, 7.91333150e-03],\n",
       "       ...,\n",
       "       [1.02347167e-05, 5.06844096e-08, 3.48560694e-11, ...,\n",
       "        4.26875442e-01, 3.14823788e-04, 3.88961844e-03],\n",
       "       [1.26333771e-06, 3.55623397e-09, 2.52151165e-13, ...,\n",
       "        1.45974874e-01, 1.12772497e-04, 2.32446170e-03],\n",
       "       [4.22254516e-06, 2.99279108e-08, 7.45401223e-12, ...,\n",
       "        3.53763252e-01, 8.36234249e-05, 8.29309784e-03]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## keras函数式API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1000/1000 [==============================] - 0s 242us/sample - loss: 12.3133 - accuracy: 0.1060\n",
      "Epoch 2/5\n",
      "1000/1000 [==============================] - 0s 67us/sample - loss: 17.5539 - accuracy: 0.1070\n",
      "Epoch 3/5\n",
      "1000/1000 [==============================] - 0s 67us/sample - loss: 32.0443 - accuracy: 0.0990\n",
      "Epoch 4/5\n",
      "1000/1000 [==============================] - 0s 64us/sample - loss: 60.1526 - accuracy: 0.1020\n",
      "Epoch 5/5\n",
      "1000/1000 [==============================] - 0s 60us/sample - loss: 102.4671 - accuracy: 0.0970\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fba780ec358>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_x = tf.keras.Input(shape=(72,))\n",
    "hidden1 = layers.Dense(32, activation='relu')(input_x)\n",
    "hidden2 = layers.Dense(16, activation='relu')(hidden1)\n",
    "pred = layers.Dense(10, activation='softmax')(hidden2)\n",
    "\n",
    "model = tf.keras.Model(inputs=input_x, outputs=pred)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "              loss=tf.keras.losses.categorical_crossentropy,\n",
    "              metrics=['accuracy'])\n",
    "model.fit(train_x, train_y, batch_size=32, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型子类化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 0s 299us/sample - loss: 13.6494 - accuracy: 0.1050\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 0s 121us/sample - loss: 17.9146 - accuracy: 0.0980\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 0s 119us/sample - loss: 21.7370 - accuracy: 0.0990\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 0s 117us/sample - loss: 24.3099 - accuracy: 0.0870\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 0s 115us/sample - loss: 26.4952 - accuracy: 0.0850\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 0s 113us/sample - loss: 27.0150 - accuracy: 0.0980\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 0s 112us/sample - loss: 26.3332 - accuracy: 0.0850\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 0s 112us/sample - loss: 25.3105 - accuracy: 0.0940\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 0s 115us/sample - loss: 24.6762 - accuracy: 0.0910\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 0s 110us/sample - loss: 24.7764 - accuracy: 0.0870\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fba107d3da0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(MyModel, self).__init__(name='my_model')\n",
    "        self.num_classes = num_classes\n",
    "        self.layer1 = layers.Dense(32, activation='relu')\n",
    "        self.layer2 = layers.Dense(num_classes, activation='softmax')\n",
    "    def call(self, inputs):\n",
    "        h1 = self.layer1(inputs)\n",
    "        out = self.layer2(h1)\n",
    "        return out\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        shape = tf.TensorShape(input_shape).as_list()\n",
    "        shape[-1] = self.num_classes\n",
    "        return tf.TensorShape(shape)\n",
    "\n",
    "model = MyModel()\n",
    "model.compile(optimizer=tf.keras.optimizers.RMSprop(0.001),\n",
    "              loss=tf.keras.losses.categorical_crossentropy,\n",
    "              metrics=['accuracy'])\n",
    "model.fit(train_x, train_y, batch_size=16, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 回调"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples, validate on 200 samples\n",
      "Epoch 1/5\n",
      "1000/1000 [==============================] - 0s 155us/sample - loss: 28.6291 - accuracy: 0.0900 - val_loss: 28.0314 - val_accuracy: 0.0800\n",
      "Epoch 2/5\n",
      "1000/1000 [==============================] - 0s 135us/sample - loss: 28.9576 - accuracy: 0.0850 - val_loss: 27.8878 - val_accuracy: 0.0800\n",
      "Epoch 3/5\n",
      "1000/1000 [==============================] - 0s 136us/sample - loss: 29.0269 - accuracy: 0.0830 - val_loss: 28.9413 - val_accuracy: 0.1000\n",
      "Epoch 4/5\n",
      "1000/1000 [==============================] - 0s 136us/sample - loss: 29.5299 - accuracy: 0.0820 - val_loss: 29.1710 - val_accuracy: 0.0950\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fba106c6358>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(patience=2, monitor='val_loss'),\n",
    "    tf.keras.callbacks.TensorBoard(log_dir='/home/yzk/logs')\n",
    "]\n",
    "model.fit(train_x, train_y, batch_size=16, epochs=5,\n",
    "          callbacks=callbacks, validation_data=(val_x, val_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多输入多输出模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset tf_flowers (218.21 MiB) to /home/yzk/tensorflow_datasets/tf_flowers/1.0.0...\u001b[0m\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "IntProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/.conda/envs/py36/lib/python3.6/site-packages/tqdm/notebook.py\u001b[0m in \u001b[0;36mstatus_printer\u001b[0;34m(_, total, desc, ncols)\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# No total? Show info style bar with no progress tqdm status\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m                 \u001b[0mpbar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mIntProgress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m                 \u001b[0mpbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'IntProgress' is not defined",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-4ef3a9ef9e0e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tf_flowers\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtfds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSplit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_supervised\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m255.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapplications\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMobileNetV2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py36/lib/python3.6/site-packages/tensorflow_datasets/core/api_utils.py\u001b[0m in \u001b[0;36mdisallow_positional_args_dec\u001b[0;34m(fn, instance, args, kwargs)\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0m_check_no_positional\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mismethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallowed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallowed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0m_check_required\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mdisallow_positional_args_dec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=no-value-for-parameter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py36/lib/python3.6/site-packages/tensorflow_datasets/core/registered.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, split, data_dir, batch_size, in_memory, shuffle_files, download, as_supervised, decoders, with_info, builder_kwargs, download_and_prepare_kwargs, as_dataset_kwargs, try_gcs)\u001b[0m\n\u001b[1;32m    298\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[0mdownload_and_prepare_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdownload_and_prepare_kwargs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m     \u001b[0mdbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload_and_prepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mdownload_and_prepare_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mas_dataset_kwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py36/lib/python3.6/site-packages/tensorflow_datasets/core/api_utils.py\u001b[0m in \u001b[0;36mdisallow_positional_args_dec\u001b[0;34m(fn, instance, args, kwargs)\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0m_check_no_positional\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mismethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallowed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallowed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0m_check_required\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mdisallow_positional_args_dec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=no-value-for-parameter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py36/lib/python3.6/site-packages/tensorflow_datasets/core/dataset_builder.py\u001b[0m in \u001b[0;36mdownload_and_prepare\u001b[0;34m(self, download_dir, download_config)\u001b[0m\n\u001b[1;32m    285\u001b[0m         self._download_and_prepare(\n\u001b[1;32m    286\u001b[0m             \u001b[0mdl_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdl_manager\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m             download_config=download_config)\n\u001b[0m\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m         \u001b[0;31m# NOTE: If modifying the lines below to put additional information in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py36/lib/python3.6/site-packages/tensorflow_datasets/core/dataset_builder.py\u001b[0m in \u001b[0;36m_download_and_prepare\u001b[0;34m(self, dl_manager, download_config)\u001b[0m\n\u001b[1;32m    946\u001b[0m     super(GeneratorBasedBuilder, self)._download_and_prepare(\n\u001b[1;32m    947\u001b[0m         \u001b[0mdl_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdl_manager\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mmax_examples_per_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdownload_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_examples_per_split\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m     )\n\u001b[1;32m    950\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py36/lib/python3.6/site-packages/tensorflow_datasets/core/dataset_builder.py\u001b[0m in \u001b[0;36m_download_and_prepare\u001b[0;34m(self, dl_manager, **prepare_split_kwargs)\u001b[0m\n\u001b[1;32m    802\u001b[0m     \u001b[0;31m# Generating data for all splits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    803\u001b[0m     \u001b[0msplit_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplits_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSplitDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 804\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0msplit_generator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_split_generators\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdl_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    805\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0msplits_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSplit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mALL\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0msplit_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m         raise ValueError(\n",
      "\u001b[0;32m~/.conda/envs/py36/lib/python3.6/site-packages/tensorflow_datasets/image/flowers.py\u001b[0m in \u001b[0;36m_split_generators\u001b[0;34m(self, dl_manager)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_split_generators\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdl_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdl_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload_and_extract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_URL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;31m# There is no predefined train/val/test split for this dataset.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py36/lib/python3.6/site-packages/tensorflow_datasets/core/download/download_manager.py\u001b[0m in \u001b[0;36mdownload_and_extract\u001b[0;34m(self, url_or_urls)\u001b[0m\n\u001b[1;32m    355\u001b[0m     \"\"\"\n\u001b[1;32m    356\u001b[0m     \u001b[0;31m# Add progress bar to follow the download state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_downloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extractor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_map_promise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_download_extract\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl_or_urls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py36/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't yield\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py36/lib/python3.6/site-packages/tensorflow_datasets/core/download/downloader.py\u001b[0m in \u001b[0;36mtqdm\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;34m\"\"\"Add a progression bar for the current download.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0masync_tqdm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_tqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0masync_tqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Dl Completed...'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m' url'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpbar_url\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0masync_tqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Dl Size...'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m' MiB'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpbar_dl_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pbar_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpbar_url\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py36/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't yield\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py36/lib/python3.6/site-packages/tensorflow_datasets/core/utils/tqdm_utils.py\u001b[0m in \u001b[0;36m_async_tqdm\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0mpbar\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAsync\u001b[0m \u001b[0mpbar\u001b[0m \u001b[0mwhich\u001b[0m \u001b[0mcan\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mshared\u001b[0m \u001b[0mbetween\u001b[0m \u001b[0mthreads\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m   \"\"\"\n\u001b[0;32m---> 98\u001b[0;31m   \u001b[0;32mwith\u001b[0m \u001b[0mtqdm_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpbar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m     \u001b[0mpbar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_TqdmPbarAsync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpbar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32myield\u001b[0m \u001b[0mpbar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py36/lib/python3.6/site-packages/tqdm/notebook.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0munit_scale\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m         self.container = self.status_printer(\n\u001b[0;32m--> 213\u001b[0;31m             self.fp, total, self.desc, self.ncols)\n\u001b[0m\u001b[1;32m    214\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py36/lib/python3.6/site-packages/tqdm/notebook.py\u001b[0m in \u001b[0;36mstatus_printer\u001b[0;34m(_, total, desc, ncols)\u001b[0m\n\u001b[1;32m    109\u001b[0m             \u001b[0;31m# #187 #451 #558\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m             raise ImportError(\n\u001b[0;32m--> 111\u001b[0;31m                 \u001b[0;34m\"IntProgress not found. Please update jupyter and ipywidgets.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m                 \u001b[0;34m\" See https://ipywidgets.readthedocs.io/en/stable\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m                 \"/user_install.html\")\n",
      "\u001b[0;31mImportError\u001b[0m: IntProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "num_batches = 1000\n",
    "batch_size = 50\n",
    "learning_rate = 0.001\n",
    "\n",
    "dataset = tfds.load(\"tf_flowers\", split=tfds.Split.TRAIN, as_supervised=True)\n",
    "dataset = dataset.map(lambda img, label: (tf.image.resize(img, [224, 224]) / 255.0, label)).shuffle(1024).batch(32)\n",
    "model = tf.keras.applications.MobileNetV2(weights=None, classes=5)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "for images, labels in dataset:\n",
    "    with tf.GradientTape() as tape:\n",
    "        labels_pred = model(images)\n",
    "        loss = tf.keras.losses.sparse_categorical_crossentropy(y_true=labels, y_pred=labels_pred)\n",
    "        loss = tf.reduce_mean(loss)\n",
    "        print(\"loss %f\" % loss.numpy())\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(grads_and_vars=zip(grads, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.dense1 = tf.keras.layers.Dense(units=24, activation=tf.nn.relu)\n",
    "        self.dense2 = tf.keras.layers.Dense(units=24, activation=tf.nn.relu)\n",
    "        self.dense3 = tf.keras.layers.Dense(units=2)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.dense1(inputs)\n",
    "        x = self.dense2(x)\n",
    "        x = self.dense3(x)\n",
    "        return x\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        q_values = self(inputs)\n",
    "        return tf.argmax(q_values, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/yzk/dist/BaseLib'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1014 09:31:48.207993 140389822482176 base_layer.py:1814] Layer q_network_18 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 3, epsilon 0.010000, score 46\n",
      "episode 4, epsilon 0.010000, score 41\n",
      "episode 7, epsilon 0.010000, score 45\n",
      "episode 11, epsilon 0.010000, score 17\n",
      "episode 12, epsilon 0.010000, score 15\n",
      "episode 13, epsilon 0.010000, score 40\n",
      "episode 14, epsilon 0.010000, score 35\n",
      "episode 18, epsilon 0.010000, score 43\n",
      "episode 19, epsilon 0.010000, score 44\n",
      "episode 20, epsilon 0.010000, score 37\n",
      "episode 22, epsilon 0.010000, score 34\n",
      "episode 23, epsilon 0.010000, score 37\n",
      "episode 24, epsilon 0.010000, score 49\n",
      "episode 25, epsilon 0.010000, score 36\n",
      "episode 26, epsilon 0.010000, score 38\n",
      "episode 27, epsilon 0.010000, score 46\n",
      "episode 31, epsilon 0.010000, score 48\n",
      "episode 32, epsilon 0.010000, score 36\n",
      "episode 33, epsilon 0.010000, score 31\n",
      "episode 34, epsilon 0.010000, score 44\n",
      "episode 35, epsilon 0.010000, score 49\n",
      "episode 39, epsilon 0.010000, score 48\n",
      "episode 40, epsilon 0.010000, score 45\n",
      "episode 41, epsilon 0.010000, score 46\n",
      "episode 42, epsilon 0.010000, score 32\n",
      "episode 43, epsilon 0.010000, score 46\n",
      "episode 44, epsilon 0.010000, score 47\n",
      "episode 46, epsilon 0.010000, score 47\n",
      "episode 48, epsilon 0.010000, score 33\n",
      "episode 49, epsilon 0.010000, score 38\n",
      "episode 51, epsilon 0.010000, score 33\n",
      "episode 53, epsilon 0.010000, score 28\n",
      "episode 57, epsilon 0.010000, score 23\n",
      "episode 58, epsilon 0.010000, score 33\n",
      "episode 59, epsilon 0.010000, score 35\n",
      "episode 60, epsilon 0.010000, score 34\n",
      "episode 61, epsilon 0.010000, score 26\n",
      "episode 62, epsilon 0.010000, score 44\n",
      "episode 63, epsilon 0.010000, score 29\n",
      "episode 67, epsilon 0.010000, score 35\n",
      "episode 69, epsilon 0.010000, score 43\n",
      "episode 70, epsilon 0.010000, score 39\n",
      "episode 71, epsilon 0.010000, score 40\n",
      "episode 72, epsilon 0.010000, score 33\n",
      "episode 73, epsilon 0.010000, score 27\n",
      "episode 74, epsilon 0.010000, score 49\n",
      "episode 76, epsilon 0.010000, score 25\n",
      "episode 77, epsilon 0.010000, score 48\n",
      "episode 83, epsilon 0.010000, score 47\n",
      "episode 85, epsilon 0.010000, score 38\n",
      "episode 91, epsilon 0.010000, score 42\n",
      "episode 93, epsilon 0.010000, score 25\n",
      "episode 94, epsilon 0.010000, score 29\n",
      "episode 99, epsilon 0.010000, score 35\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from collections import deque\n",
    "import random\n",
    "import numpy as np\n",
    "env = gym.make('CartPole-v1')       # 实例化一个游戏环境，参数为游戏名称\n",
    "model = QNetwork()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "replay_buffer = deque(maxlen=10000) # 使用一个 deque 作为 Q Learning 的经验回放池\n",
    "initial_epsilon= 0.001\n",
    "final_epsilon = 0.01\n",
    "epsilon = initial_epsilon\n",
    "num_episodes = 100\n",
    "num_exploration_episodes = 10\n",
    "max_len_episode = 50\n",
    "gamma = 0.01\n",
    "tf.keras.backend.set_floatx('float64')\n",
    "summary_writer = tf.summary.create_file_writer('./tensorboard') \n",
    "for episode_id in range(num_episodes):\n",
    "    state = env.reset()             # 初始化环境，获得初始状态\n",
    "    epsilon = max(                  # 计算当前探索率\n",
    "        initial_epsilon * (num_exploration_episodes - episode_id) / num_exploration_episodes,\n",
    "        final_epsilon)\n",
    "    for t in range(max_len_episode):\n",
    "#         env.render()                                # 对当前帧进行渲染，绘图到屏幕\n",
    "        if random.random() < epsilon:               # epsilon-greedy 探索策略，以 epsilon 的概率选择随机动作\n",
    "            action = env.action_space.sample()      # 选择随机动作（探索）\n",
    "        else:\n",
    "            action = model.predict(np.expand_dims(state, axis=0)).numpy()   # 选择模型计算出的 Q Value 最大的动作\n",
    "            action = action[0]\n",
    "\n",
    "        # 让环境执行动作，获得执行完动作的下一个状态，动作的奖励，游戏是否已结束以及额外信息\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        # 如果游戏Game Over，给予大的负奖励\n",
    "        reward = -10. if done else reward\n",
    "        # 将(state, action, reward, next_state)的四元组（外加 done 标签表示是否结束）放入经验回放池\n",
    "        replay_buffer.append((state, action, reward, next_state, 1 if done else 0))\n",
    "        # 更新当前 state\n",
    "        state = next_state\n",
    "\n",
    "        if done:                                    # 游戏结束则退出本轮循环，进行下一个 episode\n",
    "            print(\"episode %d, epsilon %f, score %d\" % (episode_id, epsilon, t))\n",
    "            break\n",
    "\n",
    "        if len(replay_buffer) >= batch_size:\n",
    "            # 从经验回放池中随机取一个批次的四元组，并分别转换为 NumPy 数组\n",
    "            batch_state, batch_action, batch_reward, batch_next_state, batch_done = zip(\n",
    "                *random.sample(replay_buffer, batch_size))\n",
    "            batch_state, batch_reward, batch_next_state, batch_done = \\\n",
    "                [np.array(a, dtype=np.float32) for a in [batch_state, batch_reward, batch_next_state, batch_done]]\n",
    "            batch_action = np.array(batch_action, dtype=np.int32)\n",
    "            \n",
    "            q_value = model(batch_next_state)\n",
    "            y = batch_reward + (gamma * tf.reduce_max(q_value, axis=1)) * (1 - batch_done)  # 计算 y 值\n",
    "            with tf.GradientTape() as tape:\n",
    "                loss = tf.keras.losses.mean_squared_error(  # 最小化 y 和 Q-value 的距离\n",
    "                    y_true=y,\n",
    "                    y_pred=tf.reduce_sum(model(batch_state) * tf.one_hot(batch_action, depth=2), axis=1)\n",
    "                )\n",
    "            \n",
    "            with summary_writer.as_default():\n",
    "                tf.summary.scalar('loss', loss, step=episode_id)\n",
    "                tf.summary.scalar('reward', batch_reward.sum(), step=episode_id)\n",
    "            grads = tape.gradient(loss, model.variables)\n",
    "            optimizer.apply_gradients(grads_and_vars=zip(grads, model.variables))       # 计算梯度并"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tensorflow_probability as tfp\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAME = 'Pendulum-v0'\n",
    "env = gym.make(GAME)\n",
    "N_A = env.action_space.shape[0]\n",
    "N_S = env.observation_space.shape[0]\n",
    "A_BOUND = [env.action_space.low, env.action_space.high]\n",
    "ENTROPY_BETA = 0.01\n",
    "\n",
    "class ACNet(tf.keras.Model):\n",
    "    def __init__(self, globalAC=None):\n",
    "        super().__init__()\n",
    "        self.globalAC = globalAC\n",
    "        self.la = tf.keras.layers.Dense(units=200, activation=tf.nn.relu)\n",
    "        self.mu = tf.keras.layers.Dense(units=N_A, activation=tf.nn.tanh)  \n",
    "        self.sigma = tf.keras.layers.Dense(units=N_A, activation=tf.nn.softplus)\n",
    "        self.lc = tf.keras.layers.Dense(100, activation=tf.nn.relu)\n",
    "        self.v = tf.keras.layers.Dense(N_A, activation=tf.nn.softmax)\n",
    "            \n",
    "    def call(self, state):\n",
    "        x = self.la(state)\n",
    "        mu = self.mu(x)\n",
    "        sigma = self.sigma(x)\n",
    "        v_s = self.lc(state)\n",
    "        self.v_s = self.v(v_s)\n",
    "        tf.multiply(mu, A_BOUND[1])\n",
    "        sigma = sigma + 1e-4\n",
    "        self.norm_dist = tfp.distributions.Normal(mu, sigma)\n",
    "        return mu, sigma, self.v_s\n",
    "    \n",
    "    def choose_action(self, state):\n",
    "        self(state)\n",
    "        return tf.clip_by_value(self.norm_dist.sample(1), A_BOUND[0], A_BOUND[1])\n",
    "    \n",
    "    def update_global(self, state, a_his, v_target):\n",
    "        mu, sigma, v_s = self(state)\n",
    "        td = tf.subtract(v_target, v_s)\n",
    "        \n",
    "        entropy = self.norm_dist.entropy()\n",
    "        a_prob = self.norm_dist.log_prob(a_his)\n",
    "        \n",
    "        exp_v = a_prob * tf.stop_gradient(td) + entropy * ENTROPY_BETA\n",
    "        \n",
    "#         update global net's paramerter\n",
    "        with tf.GradientTape() as tape:\n",
    "            c_loss = tf.reduce_mean(tf.square(td))\n",
    "        grad_c = tape.gradient(c_loss, self.variables)\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            a_loss = tf.reduce_mean(-exp_v)\n",
    "        grad_a = tape.gradient(a_loss, self.variables)\n",
    "        \n",
    "        OPT_C.apply_gradients(grads_and_vars=zip(grad_c, self.globalAC.variables))\n",
    "        OPT_A.apply_gradients(grads_and_vars=zip(grad_a, self.globalAC.variables))\n",
    "        \n",
    "    def pull_global(self):\n",
    "        for l_param, g_param in zip(self.variables, self.globalAC.variables):\n",
    "            l_param.assign(g_param)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = ACNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.41219156,  0.9110972 ,  0.60386164])"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = env.reset()\n",
    "a = env.action_space.sample()\n",
    "s_, r, done, info = env.step(a)\n",
    "s_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1014 16:10:09.738444 140208464201472 base_layer.py:1814] Layer ac_net_1046 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=349293, shape=(1, 1), dtype=float32, numpy=array([[1.0151976]], dtype=float32)>"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = net.choose_action(s_.reshape((1, -1))).numpy().ravel()\n",
    "net.norm_dist.entropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_GLOBAL_EP = 2000\n",
    "MAX_EPISODE = 200\n",
    "UPDATE_GLOBAL_ITER = 10\n",
    "GLOABL_REWARD_R = []\n",
    "GLOBAL_EP = 0\n",
    "GAMMA = 0.9\n",
    "        \n",
    "class Worker():\n",
    "    def __init__(self,name, globalAC):\n",
    "        self.env = gym.make(GAME).unwrapped\n",
    "        self.name = name \n",
    "        self.ACNet = ACNet(globalAC)\n",
    "    def work(self):\n",
    "        total_step = 1\n",
    "        buffer_s, buffer_a, buffer_r = [], [], []\n",
    "        global GLOBAL_EP, GLOABL_REWARD_R\n",
    "        while not COORD.should_stop() and GLOBAL_EP < MAX_GLOBAL_EP:\n",
    "            ep_r = 0\n",
    "            s = self.env.reset()\n",
    "            for ep_t in range(MAX_EPISODE):\n",
    "                a = self.ACNet.choose_action(s.reshape(1, -1)).numpy().ravel()\n",
    "                s_ , r, done, info = self.env.step(a)\n",
    "                ep_r += r\n",
    "                done = True if ep_t == MAX_EPISODE -1 else False\n",
    "                buffer_s.append(s)\n",
    "                buffer_a.append(a)\n",
    "                buffer_r.append((r +  8)/ 8)\n",
    "                if total_step % UPDATE_GLOBAL_ITER ==0 or done:\n",
    "                    if done:\n",
    "                        v_s_ = 0\n",
    "                    else:\n",
    "                        v_s_ = self.ACNet(s_.reshape(1, -1))[-1].numpy().ravel()\n",
    "                    buffer_v_target = []\n",
    "                    for item in buffer_r[::-1]:\n",
    "                        v_s_  = v_s_ * GAMMA + item\n",
    "                        buffer_v_target.append(v_s_)\n",
    "                    buffer_v_target.reverse()\n",
    "                    buffer_s, buffer_a, buffer_v_target = np.vstack(buffer_s), np.vstack(buffer_a), np.vstack(buffer_v_target)\n",
    "                    self.ACNet.update_global(buffer_s, buffer_a, buffer_v_target)\n",
    "                    buffer_a,buffer_s,buffer_r = [],[],[]\n",
    "                    self.ACNet.pull_global()\n",
    "\n",
    "                total_step += 1\n",
    "                s = s_\n",
    "                if done:\n",
    "                    if len(GLOABL_REWARD_R) == 0:\n",
    "                        GLOABL_REWARD_R.append(ep_r)\n",
    "                    else:\n",
    "                        GLOABL_REWARD_R.append(GLOABL_REWARD_R[-1]*0.9 + 0.1*ep_r)\n",
    "                    print(self.name, \"EP:\",GLOBAL_EP, \"Reward:\", GLOABL_REWARD_R[-1])\n",
    "                    GLOBAL_EP +=1    \n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1014 16:14:14.490692 140186096748288 base_layer.py:1814] Layer ac_net_1105 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "W1014 16:14:14.492889 140186088355584 base_layer.py:1814] Layer ac_net_1106 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "W1014 16:14:14.494340 140185534699264 base_layer.py:1814] Layer ac_net_1107 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "W1014 16:14:14.495753 140187220813568 base_layer.py:1814] Layer ac_net_1108 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "W1014 16:14:14.497171 140190383322880 base_layer.py:1814] Layer ac_net_1109 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "W1014 16:14:14.498579 140189703841536 base_layer.py:1814] Layer ac_net_1110 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "W1014 16:14:14.499984 140189695448832 base_layer.py:1814] Layer ac_net_1111 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "W1014 16:14:14.501986 140188940498688 base_layer.py:1814] Layer ac_net_1112 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "W1014 16:14:14.503859 140188932105984 base_layer.py:1814] Layer ac_net_1113 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "W1014 16:14:14.505350 140188923713280 base_layer.py:1814] Layer ac_net_1114 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "W1014 16:14:14.506741 140188915320576 base_layer.py:1814] Layer ac_net_1115 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1014 16:14:14.508249 140188906927872 base_layer.py:1814] Layer ac_net_1116 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "W1014 16:14:14.509714 140188898535168 base_layer.py:1814] Layer ac_net_1117 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "W1014 16:14:14.511181 140188890142464 base_layer.py:1814] Layer ac_net_1118 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "W1014 16:14:14.513389 140188336518912 base_layer.py:1814] Layer ac_net_1119 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "W1014 16:14:14.514816 140188328126208 base_layer.py:1814] Layer ac_net_1120 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "W1014 16:14:14.516251 140188319733504 base_layer.py:1814] Layer ac_net_1121 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "W1014 16:14:14.517698 140188311340800 base_layer.py:1814] Layer ac_net_1122 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "W1014 16:14:14.519144 140188302948096 base_layer.py:1814] Layer ac_net_1123 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "W1014 16:14:14.520644 140188294555392 base_layer.py:1814] Layer ac_net_1124 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "W1014 16:14:14.522273 140188286162688 base_layer.py:1814] Layer ac_net_1125 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "W1014 16:14:14.524261 140187799648000 base_layer.py:1814] Layer ac_net_1126 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1014 16:14:14.525701 140187791255296 base_layer.py:1814] Layer ac_net_1127 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "W1014 16:14:14.527138 140187782862592 base_layer.py:1814] Layer ac_net_1128 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "W1014 16:14:14.528622 140187774469888 base_layer.py:1814] Layer ac_net_1129 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "W1014 16:14:14.530063 140187766077184 base_layer.py:1814] Layer ac_net_1130 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "W1014 16:14:14.533101 140187757684480 base_layer.py:1814] Layer ac_net_1131 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "W1014 16:14:14.534578 140187749291776 base_layer.py:1814] Layer ac_net_1132 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "W1014 16:14:14.536684 140187262777088 base_layer.py:1814] Layer ac_net_1133 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "W1014 16:14:14.538185 140187254384384 base_layer.py:1814] Layer ac_net_1134 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "W1014 16:14:14.539637 140187245991680 base_layer.py:1814] Layer ac_net_1135 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "W1014 16:14:14.541190 140187237598976 base_layer.py:1814] Layer ac_net_1136 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "W1014 16:14:14.542847 140187229206272 base_layer.py:1814] Layer ac_net_1137 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1014 16:14:14.544237 140187212420864 base_layer.py:1814] Layer ac_net_1138 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "W1014 16:14:14.546352 140186658797312 base_layer.py:1814] Layer ac_net_1139 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "W1014 16:14:14.547807 140186650404608 base_layer.py:1814] Layer ac_net_1140 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "W1014 16:14:14.549322 140186642011904 base_layer.py:1814] Layer ac_net_1141 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "W1014 16:14:14.550808 140186633619200 base_layer.py:1814] Layer ac_net_1142 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "W1014 16:14:14.552476 140186625226496 base_layer.py:1814] Layer ac_net_1143 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "W1014 16:14:14.554033 140186616833792 base_layer.py:1814] Layer ac_net_1144 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "W1014 16:14:14.555498 140186608441088 base_layer.py:1814] Layer ac_net_1145 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "W1014 16:14:14.557677 140186121926400 base_layer.py:1814] Layer ac_net_1146 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "W1014 16:14:14.559164 140186113533696 base_layer.py:1814] Layer ac_net_1147 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "W1014 16:14:14.560883 140186105140992 base_layer.py:1814] Layer ac_net_1148 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1014 16:14:14.562362 140186079962880 base_layer.py:1814] Layer ac_net_1149 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "W1014 16:14:14.563882 140186071570176 base_layer.py:1814] Layer ac_net_1150 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "W1014 16:14:14.566123 140185585055488 base_layer.py:1814] Layer ac_net_1151 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "W1014 16:14:14.567637 140185576662784 base_layer.py:1814] Layer ac_net_1152 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "W1014 16:14:14.569156 140185568270080 base_layer.py:1814] Layer ac_net_1153 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "W1014 16:14:14.570883 140185559877376 base_layer.py:1814] Layer ac_net_1154 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "W1014 16:14:14.572381 140185551484672 base_layer.py:1814] Layer ac_net_1155 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "W1014 16:14:14.576464 140185543091968 base_layer.py:1814] Layer ac_net_1156 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "W1014 16:14:14.580530 140185048184576 base_layer.py:1814] Layer ac_net_1157 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "W1014 16:14:14.582035 140185039791872 base_layer.py:1814] Layer ac_net_1158 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "W1014 16:14:14.583673 140185031399168 base_layer.py:1814] Layer ac_net_1159 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1014 16:14:14.586509 140185023006464 base_layer.py:1814] Layer ac_net_1160 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Exception in thread Thread-1049:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/yzk/.conda/envs/py36/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/yzk/.conda/envs/py36/lib/python3.6/threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-204-3a4fe299d869>\", line 19, in <lambda>\n",
      "    job = lambda: worker.work()\n",
      "  File \"<ipython-input-203-3ef2f5ae018a>\", line 46, in work\n",
      "    if len(GLOABL_REWARD_R) == 0:\n",
      "NameError: name 'GLOABL_REWARD_R' is not defined\n",
      "\n",
      "Exception in thread Thread-1044:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/yzk/.conda/envs/py36/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/yzk/.conda/envs/py36/lib/python3.6/threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-204-3a4fe299d869>\", line 19, in <lambda>\n",
      "    job = lambda: worker.work()\n",
      "  File \"<ipython-input-203-3ef2f5ae018a>\", line 46, in work\n",
      "    if len(GLOABL_REWARD_R) == 0:\n",
      "NameError: name 'GLOABL_REWARD_R' is not defined\n",
      "\n",
      "Exception in thread Thread-1019:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/yzk/.conda/envs/py36/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/yzk/.conda/envs/py36/lib/python3.6/threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-204-3a4fe299d869>\", line 19, in <lambda>\n",
      "    job = lambda: worker.work()\n",
      "  File \"<ipython-input-203-3ef2f5ae018a>\", line 46, in work\n",
      "    if len(GLOABL_REWARD_R) == 0:\n",
      "NameError: name 'GLOABL_REWARD_R' is not defined\n",
      "\n",
      "Exception in thread Thread-1033:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/yzk/.conda/envs/py36/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/yzk/.conda/envs/py36/lib/python3.6/threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-204-3a4fe299d869>\", line 19, in <lambda>\n",
      "    job = lambda: worker.work()\n",
      "  File \"<ipython-input-203-3ef2f5ae018a>\", line 46, in work\n",
      "    if len(GLOABL_REWARD_R) == 0:\n",
      "NameError: name 'GLOABL_REWARD_R' is not defined\n",
      "\n",
      "Exception in thread Thread-1045:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/yzk/.conda/envs/py36/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/yzk/.conda/envs/py36/lib/python3.6/threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-204-3a4fe299d869>\", line 19, in <lambda>\n",
      "    job = lambda: worker.work()\n",
      "  File \"<ipython-input-203-3ef2f5ae018a>\", line 46, in work\n",
      "    if len(GLOABL_REWARD_R) == 0:\n",
      "NameError: name 'GLOABL_REWARD_R' is not defined\n",
      "\n",
      "Exception in thread Thread-1054:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/yzk/.conda/envs/py36/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/yzk/.conda/envs/py36/lib/python3.6/threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-204-3a4fe299d869>\", line 19, in <lambda>\n",
      "    job = lambda: worker.work()\n",
      "  File \"<ipython-input-203-3ef2f5ae018a>\", line 46, in work\n",
      "    if len(GLOABL_REWARD_R) == 0:\n",
      "NameError: name 'GLOABL_REWARD_R' is not defined\n",
      "\n",
      "Exception in thread Thread-1034:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/yzk/.conda/envs/py36/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/yzk/.conda/envs/py36/lib/python3.6/threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-204-3a4fe299d869>\", line 19, in <lambda>\n",
      "    job = lambda: worker.work()\n",
      "  File \"<ipython-input-203-3ef2f5ae018a>\", line 46, in work\n",
      "    if len(GLOABL_REWARD_R) == 0:\n",
      "NameError: name 'GLOABL_REWARD_R' is not defined\n",
      "\n",
      "Exception in thread Thread-1025:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/yzk/.conda/envs/py36/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/yzk/.conda/envs/py36/lib/python3.6/threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-204-3a4fe299d869>\", line 19, in <lambda>\n",
      "    job = lambda: worker.work()\n",
      "  File \"<ipython-input-203-3ef2f5ae018a>\", line 46, in work\n",
      "    if len(GLOABL_REWARD_R) == 0:\n",
      "NameError: name 'GLOABL_REWARD_R' is not defined\n",
      "\n",
      "Exception in thread Thread-1048:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/yzk/.conda/envs/py36/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/yzk/.conda/envs/py36/lib/python3.6/threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-204-3a4fe299d869>\", line 19, in <lambda>\n",
      "    job = lambda: worker.work()\n",
      "  File \"<ipython-input-203-3ef2f5ae018a>\", line 46, in work\n",
      "    if len(GLOABL_REWARD_R) == 0:\n",
      "NameError: name 'GLOABL_REWARD_R' is not defined\n",
      "\n",
      "Exception in thread Thread-1021:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/yzk/.conda/envs/py36/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/yzk/.conda/envs/py36/lib/python3.6/threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-204-3a4fe299d869>\", line 19, in <lambda>\n",
      "    job = lambda: worker.work()\n",
      "  File \"<ipython-input-203-3ef2f5ae018a>\", line 46, in work\n",
      "    if len(GLOABL_REWARD_R) == 0:\n",
      "NameError: name 'GLOABL_REWARD_R' is not defined\n",
      "\n",
      "Exception in thread Thread-1042:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/yzk/.conda/envs/py36/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/yzk/.conda/envs/py36/lib/python3.6/threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-204-3a4fe299d869>\", line 19, in <lambda>\n",
      "    job = lambda: worker.work()\n",
      "  File \"<ipython-input-203-3ef2f5ae018a>\", line 46, in work\n",
      "    if len(GLOABL_REWARD_R) == 0:\n",
      "NameError: name 'GLOABL_REWARD_R' is not defined\n",
      "\n",
      "Exception in thread Thread-1024:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/yzk/.conda/envs/py36/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/yzk/.conda/envs/py36/lib/python3.6/threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-204-3a4fe299d869>\", line 19, in <lambda>\n",
      "    job = lambda: worker.work()\n",
      "  File \"<ipython-input-203-3ef2f5ae018a>\", line 46, in work\n",
      "    if len(GLOABL_REWARD_R) == 0:\n",
      "NameError: name 'GLOABL_REWARD_R' is not defined\n",
      "\n",
      "Exception in thread Thread-1057:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/yzk/.conda/envs/py36/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/yzk/.conda/envs/py36/lib/python3.6/threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-204-3a4fe299d869>\", line 19, in <lambda>\n",
      "    job = lambda: worker.work()\n",
      "  File \"<ipython-input-203-3ef2f5ae018a>\", line 46, in work\n",
      "    if len(GLOABL_REWARD_R) == 0:\n",
      "NameError: name 'GLOABL_REWARD_R' is not defined\n",
      "\n",
      "Exception in thread Thread-1037:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/yzk/.conda/envs/py36/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/yzk/.conda/envs/py36/lib/python3.6/threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-204-3a4fe299d869>\", line 19, in <lambda>\n",
      "    job = lambda: worker.work()\n",
      "  File \"<ipython-input-203-3ef2f5ae018a>\", line 46, in work\n",
      "    if len(GLOABL_REWARD_R) == 0:\n",
      "NameError: name 'GLOABL_REWARD_R' is not defined\n",
      "\n",
      "Exception in thread Thread-1030:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/yzk/.conda/envs/py36/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/yzk/.conda/envs/py36/lib/python3.6/threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-204-3a4fe299d869>\", line 19, in <lambda>\n",
      "    job = lambda: worker.work()\n",
      "  File \"<ipython-input-203-3ef2f5ae018a>\", line 46, in work\n",
      "    if len(GLOABL_REWARD_R) == 0:\n",
      "NameError: name 'GLOABL_REWARD_R' is not defined\n",
      "\n",
      "Exception in thread Thread-1029:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/yzk/.conda/envs/py36/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/yzk/.conda/envs/py36/lib/python3.6/threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-204-3a4fe299d869>\", line 19, in <lambda>\n",
      "    job = lambda: worker.work()\n",
      "  File \"<ipython-input-203-3ef2f5ae018a>\", line 46, in work\n",
      "    if len(GLOABL_REWARD_R) == 0:\n",
      "NameError: name 'GLOABL_REWARD_R' is not defined\n",
      "Exception in thread Thread-1023:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/yzk/.conda/envs/py36/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/yzk/.conda/envs/py36/lib/python3.6/threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-204-3a4fe299d869>\", line 19, in <lambda>\n",
      "    job = lambda: worker.work()\n",
      "  File \"<ipython-input-203-3ef2f5ae018a>\", line 46, in work\n",
      "    if len(GLOABL_REWARD_R) == 0:\n",
      "NameError: name 'GLOABL_REWARD_R' is not defined\n",
      "\n",
      "\n",
      "Exception in thread Thread-1032:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/yzk/.conda/envs/py36/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/yzk/.conda/envs/py36/lib/python3.6/threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-204-3a4fe299d869>\", line 19, in <lambda>\n",
      "    job = lambda: worker.work()\n",
      "  File \"<ipython-input-203-3ef2f5ae018a>\", line 46, in work\n",
      "    if len(GLOABL_REWARD_R) == 0:\n",
      "NameError: name 'GLOABL_REWARD_R' is not defined\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-1028:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/yzk/.conda/envs/py36/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/yzk/.conda/envs/py36/lib/python3.6/threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-204-3a4fe299d869>\", line 19, in <lambda>\n",
      "    job = lambda: worker.work()\n",
      "  File \"<ipython-input-203-3ef2f5ae018a>\", line 46, in work\n",
      "    if len(GLOABL_REWARD_R) == 0:\n",
      "NameError: name 'GLOABL_REWARD_R' is not defined\n",
      "\n",
      "Exception in thread Thread-1061:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/yzk/.conda/envs/py36/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/yzk/.conda/envs/py36/lib/python3.6/threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-204-3a4fe299d869>\", line 19, in <lambda>\n",
      "    job = lambda: worker.work()\n",
      "  File \"<ipython-input-203-3ef2f5ae018a>\", line 46, in work\n",
      "    if len(GLOABL_REWARD_R) == 0:\n",
      "NameError: name 'GLOABL_REWARD_R' is not defined\n",
      "\n",
      "Exception in thread Thread-1062:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/yzk/.conda/envs/py36/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/yzk/.conda/envs/py36/lib/python3.6/threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-204-3a4fe299d869>\", line 19, in <lambda>\n",
      "    job = lambda: worker.work()\n",
      "  File \"<ipython-input-203-3ef2f5ae018a>\", line 46, in work\n",
      "    if len(GLOABL_REWARD_R) == 0:\n",
      "NameError: name 'GLOABL_REWARD_R' is not defined\n",
      "\n",
      "Exception in thread Thread-1027:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/yzk/.conda/envs/py36/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/yzk/.conda/envs/py36/lib/python3.6/threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-204-3a4fe299d869>\", line 19, in <lambda>\n",
      "    job = lambda: worker.work()\n",
      "  File \"<ipython-input-203-3ef2f5ae018a>\", line 46, in work\n",
      "    if len(GLOABL_REWARD_R) == 0:\n",
      "NameError: name 'GLOABL_REWARD_R' is not defined\n",
      "\n",
      "Exception in thread Thread-1020:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/yzk/.conda/envs/py36/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/yzk/.conda/envs/py36/lib/python3.6/threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-204-3a4fe299d869>\", line 19, in <lambda>\n",
      "    job = lambda: worker.work()\n",
      "  File \"<ipython-input-203-3ef2f5ae018a>\", line 46, in work\n",
      "    if len(GLOABL_REWARD_R) == 0:\n",
      "NameError: name 'GLOABL_REWARD_R' is not defined\n",
      "Exception in thread Thread-1052:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/yzk/.conda/envs/py36/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/yzk/.conda/envs/py36/lib/python3.6/threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-204-3a4fe299d869>\", line 19, in <lambda>\n",
      "    job = lambda: worker.work()\n",
      "  File \"<ipython-input-203-3ef2f5ae018a>\", line 46, in work\n",
      "    if len(GLOABL_REWARD_R) == 0:\n",
      "NameError: name 'GLOABL_REWARD_R' is not defined\n",
      "\n",
      "\n",
      "Exception in thread Thread-1053:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/yzk/.conda/envs/py36/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/yzk/.conda/envs/py36/lib/python3.6/threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-204-3a4fe299d869>\", line 19, in <lambda>\n",
      "    job = lambda: worker.work()\n",
      "  File \"<ipython-input-203-3ef2f5ae018a>\", line 46, in work\n",
      "    if len(GLOABL_REWARD_R) == 0:\n",
      "NameError: name 'GLOABL_REWARD_R' is not defined\n",
      "\n",
      "Exception in thread Thread-1058:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/yzk/.conda/envs/py36/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/yzk/.conda/envs/py36/lib/python3.6/threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-204-3a4fe299d869>\", line 19, in <lambda>\n",
      "    job = lambda: worker.work()\n",
      "  File \"<ipython-input-203-3ef2f5ae018a>\", line 46, in work\n",
      "    if len(GLOABL_REWARD_R) == 0:\n",
      "NameError: name 'GLOABL_REWARD_R' is not defined\n",
      "\n",
      "Exception in thread Thread-1046:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/yzk/.conda/envs/py36/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/yzk/.conda/envs/py36/lib/python3.6/threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-204-3a4fe299d869>\", line 19, in <lambda>\n",
      "    job = lambda: worker.work()\n",
      "  File \"<ipython-input-203-3ef2f5ae018a>\", line 46, in work\n",
      "    if len(GLOABL_REWARD_R) == 0:\n",
      "NameError: name 'GLOABL_REWARD_R' is not defined\n",
      "\n",
      "Exception in thread Thread-1056:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/yzk/.conda/envs/py36/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/yzk/.conda/envs/py36/lib/python3.6/threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-204-3a4fe299d869>\", line 19, in <lambda>\n",
      "    job = lambda: worker.work()\n",
      "  File \"<ipython-input-203-3ef2f5ae018a>\", line 46, in work\n",
      "    if len(GLOABL_REWARD_R) == 0:\n",
      "NameError: name 'GLOABL_REWARD_R' is not defined\n",
      "\n",
      "Exception in thread Thread-1035:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/yzk/.conda/envs/py36/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/yzk/.conda/envs/py36/lib/python3.6/threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-204-3a4fe299d869>\", line 19, in <lambda>\n",
      "    job = lambda: worker.work()\n",
      "  File \"<ipython-input-203-3ef2f5ae018a>\", line 46, in work\n",
      "    if len(GLOABL_REWARD_R) == 0:\n",
      "NameError: name 'GLOABL_REWARD_R' is not defined\n",
      "\n",
      "Exception in thread Thread-1036:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/yzk/.conda/envs/py36/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/yzk/.conda/envs/py36/lib/python3.6/threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-204-3a4fe299d869>\", line 19, in <lambda>\n",
      "    job = lambda: worker.work()\n",
      "  File \"<ipython-input-203-3ef2f5ae018a>\", line 46, in work\n",
      "    if len(GLOABL_REWARD_R) == 0:\n",
      "NameError: name 'GLOABL_REWARD_R' is not defined\n",
      "\n",
      "Exception in thread Thread-1055:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/yzk/.conda/envs/py36/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/yzk/.conda/envs/py36/lib/python3.6/threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-204-3a4fe299d869>\", line 19, in <lambda>\n",
      "    job = lambda: worker.work()\n",
      "  File \"<ipython-input-203-3ef2f5ae018a>\", line 46, in work\n",
      "    if len(GLOABL_REWARD_R) == 0:\n",
      "NameError: name 'GLOABL_REWARD_R' is not defined\n",
      "\n",
      "Exception in thread Thread-1064:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/yzk/.conda/envs/py36/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/yzk/.conda/envs/py36/lib/python3.6/threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-204-3a4fe299d869>\", line 19, in <lambda>\n",
      "    job = lambda: worker.work()\n",
      "  File \"<ipython-input-203-3ef2f5ae018a>\", line 46, in work\n",
      "    if len(GLOABL_REWARD_R) == 0:\n",
      "NameError: name 'GLOABL_REWARD_R' is not defined\n",
      "Exception in thread Thread-1047:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/yzk/.conda/envs/py36/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/yzk/.conda/envs/py36/lib/python3.6/threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-204-3a4fe299d869>\", line 19, in <lambda>\n",
      "    job = lambda: worker.work()\n",
      "  File \"<ipython-input-203-3ef2f5ae018a>\", line 46, in work\n",
      "    if len(GLOABL_REWARD_R) == 0:\n",
      "NameError: name 'GLOABL_REWARD_R' is not defined\n",
      "\n",
      "\n",
      "Exception in thread Thread-1073:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/yzk/.conda/envs/py36/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/yzk/.conda/envs/py36/lib/python3.6/threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-204-3a4fe299d869>\", line 19, in <lambda>\n",
      "    job = lambda: worker.work()\n",
      "  File \"<ipython-input-203-3ef2f5ae018a>\", line 46, in work\n",
      "    if len(GLOABL_REWARD_R) == 0:\n",
      "NameError: name 'GLOABL_REWARD_R' is not defined\n",
      "\n",
      "Exception in thread Thread-1022:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/yzk/.conda/envs/py36/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/yzk/.conda/envs/py36/lib/python3.6/threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-204-3a4fe299d869>\", line 19, in <lambda>\n",
      "    job = lambda: worker.work()\n",
      "  File \"<ipython-input-203-3ef2f5ae018a>\", line 46, in work\n",
      "    if len(GLOABL_REWARD_R) == 0:\n",
      "NameError: name 'GLOABL_REWARD_R' is not defined\n",
      "\n",
      "Exception in thread Thread-1070:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/yzk/.conda/envs/py36/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/yzk/.conda/envs/py36/lib/python3.6/threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-204-3a4fe299d869>\", line 19, in <lambda>\n",
      "    job = lambda: worker.work()\n",
      "  File \"<ipython-input-203-3ef2f5ae018a>\", line 46, in work\n",
      "    if len(GLOABL_REWARD_R) == 0:\n",
      "NameError: name 'GLOABL_REWARD_R' is not defined\n",
      "\n",
      "Exception in thread Thread-1051:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/yzk/.conda/envs/py36/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/yzk/.conda/envs/py36/lib/python3.6/threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-204-3a4fe299d869>\", line 19, in <lambda>\n",
      "    job = lambda: worker.work()\n",
      "  File \"<ipython-input-203-3ef2f5ae018a>\", line 46, in work\n",
      "    if len(GLOABL_REWARD_R) == 0:\n",
      "NameError: name 'GLOABL_REWARD_R' is not defined\n",
      "\n",
      "Exception in thread Thread-1050:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/yzk/.conda/envs/py36/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/yzk/.conda/envs/py36/lib/python3.6/threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-204-3a4fe299d869>\", line 19, in <lambda>\n",
      "    job = lambda: worker.work()\n",
      "  File \"<ipython-input-203-3ef2f5ae018a>\", line 46, in work\n",
      "    if len(GLOABL_REWARD_R) == 0:\n",
      "NameError: name 'GLOABL_REWARD_R' is not defined\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-1038:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/yzk/.conda/envs/py36/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/yzk/.conda/envs/py36/lib/python3.6/threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-204-3a4fe299d869>\", line 19, in <lambda>\n",
      "    job = lambda: worker.work()\n",
      "  File \"<ipython-input-203-3ef2f5ae018a>\", line 46, in work\n",
      "    if len(GLOABL_REWARD_R) == 0:\n",
      "NameError: name 'GLOABL_REWARD_R' is not defined\n",
      "\n",
      "Exception in thread Thread-1069:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/yzk/.conda/envs/py36/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/yzk/.conda/envs/py36/lib/python3.6/threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-204-3a4fe299d869>\", line 19, in <lambda>\n",
      "    job = lambda: worker.work()\n",
      "  File \"<ipython-input-203-3ef2f5ae018a>\", line 46, in work\n",
      "    if len(GLOABL_REWARD_R) == 0:\n",
      "NameError: name 'GLOABL_REWARD_R' is not defined\n",
      "\n",
      "Exception in thread Thread-1039:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/yzk/.conda/envs/py36/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/yzk/.conda/envs/py36/lib/python3.6/threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-204-3a4fe299d869>\", line 19, in <lambda>\n",
      "    job = lambda: worker.work()\n",
      "  File \"<ipython-input-203-3ef2f5ae018a>\", line 46, in work\n",
      "    if len(GLOABL_REWARD_R) == 0:\n",
      "NameError: name 'GLOABL_REWARD_R' is not defined\n",
      "\n",
      "Exception in thread Thread-1067:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/yzk/.conda/envs/py36/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/yzk/.conda/envs/py36/lib/python3.6/threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-204-3a4fe299d869>\", line 19, in <lambda>\n",
      "    job = lambda: worker.work()\n",
      "  File \"<ipython-input-203-3ef2f5ae018a>\", line 46, in work\n",
      "    if len(GLOABL_REWARD_R) == 0:\n",
      "NameError: name 'GLOABL_REWARD_R' is not defined\n",
      "\n",
      "Exception in thread Thread-1043:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/yzk/.conda/envs/py36/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/yzk/.conda/envs/py36/lib/python3.6/threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-204-3a4fe299d869>\", line 19, in <lambda>\n",
      "    job = lambda: worker.work()\n",
      "  File \"<ipython-input-203-3ef2f5ae018a>\", line 46, in work\n",
      "    if len(GLOABL_REWARD_R) == 0:\n",
      "NameError: name 'GLOABL_REWARD_R' is not defined\n",
      "\n",
      "Exception in thread Thread-1068:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/yzk/.conda/envs/py36/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/yzk/.conda/envs/py36/lib/python3.6/threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-204-3a4fe299d869>\", line 19, in <lambda>\n",
      "    job = lambda: worker.work()\n",
      "  File \"<ipython-input-203-3ef2f5ae018a>\", line 46, in work\n",
      "    if len(GLOABL_REWARD_R) == 0:\n",
      "NameError: name 'GLOABL_REWARD_R' is not defined\n",
      "\n",
      "Exception in thread Thread-1071:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/yzk/.conda/envs/py36/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/yzk/.conda/envs/py36/lib/python3.6/threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-204-3a4fe299d869>\", line 19, in <lambda>\n",
      "    job = lambda: worker.work()\n",
      "  File \"<ipython-input-203-3ef2f5ae018a>\", line 46, in work\n",
      "    if len(GLOABL_REWARD_R) == 0:\n",
      "NameError: name 'GLOABL_REWARD_R' is not defined\n",
      "\n",
      "Exception in thread Thread-1041:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/yzk/.conda/envs/py36/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/yzk/.conda/envs/py36/lib/python3.6/threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-204-3a4fe299d869>\", line 19, in <lambda>\n",
      "    job = lambda: worker.work()\n",
      "  File \"<ipython-input-203-3ef2f5ae018a>\", line 46, in work\n",
      "    if len(GLOABL_REWARD_R) == 0:\n",
      "NameError: name 'GLOABL_REWARD_R' is not defined\n",
      "\n",
      "Exception in thread Thread-1072:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/yzk/.conda/envs/py36/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/yzk/.conda/envs/py36/lib/python3.6/threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-204-3a4fe299d869>\", line 19, in <lambda>\n",
      "    job = lambda: worker.work()\n",
      "  File \"<ipython-input-203-3ef2f5ae018a>\", line 46, in work\n",
      "    if len(GLOABL_REWARD_R) == 0:\n",
      "NameError: name 'GLOABL_REWARD_R' is not defined\n",
      "\n",
      "Exception in thread Thread-1066:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/yzk/.conda/envs/py36/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/yzk/.conda/envs/py36/lib/python3.6/threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-204-3a4fe299d869>\", line 19, in <lambda>\n",
      "    job = lambda: worker.work()\n",
      "  File \"<ipython-input-203-3ef2f5ae018a>\", line 46, in work\n",
      "    if len(GLOABL_REWARD_R) == 0:\n",
      "NameError: name 'GLOABL_REWARD_R' is not defined\n",
      "\n",
      "Exception in thread Thread-1065:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/yzk/.conda/envs/py36/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/yzk/.conda/envs/py36/lib/python3.6/threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-204-3a4fe299d869>\", line 19, in <lambda>\n",
      "    job = lambda: worker.work()\n",
      "  File \"<ipython-input-203-3ef2f5ae018a>\", line 46, in work\n",
      "    if len(GLOABL_REWARD_R) == 0:\n",
      "NameError: name 'GLOABL_REWARD_R' is not defined\n",
      "\n",
      "Exception in thread Thread-1040:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/yzk/.conda/envs/py36/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/yzk/.conda/envs/py36/lib/python3.6/threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-204-3a4fe299d869>\", line 19, in <lambda>\n",
      "    job = lambda: worker.work()\n",
      "  File \"<ipython-input-203-3ef2f5ae018a>\", line 46, in work\n",
      "    if len(GLOABL_REWARD_R) == 0:\n",
      "NameError: name 'GLOABL_REWARD_R' is not defined\n",
      "\n",
      "Exception in thread Thread-1026:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/yzk/.conda/envs/py36/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/yzk/.conda/envs/py36/lib/python3.6/threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-204-3a4fe299d869>\", line 19, in <lambda>\n",
      "    job = lambda: worker.work()\n",
      "  File \"<ipython-input-203-3ef2f5ae018a>\", line 46, in work\n",
      "    if len(GLOABL_REWARD_R) == 0:\n",
      "NameError: name 'GLOABL_REWARD_R' is not defined\n",
      "\n",
      "Exception in thread Thread-1031:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/yzk/.conda/envs/py36/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/yzk/.conda/envs/py36/lib/python3.6/threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-204-3a4fe299d869>\", line 19, in <lambda>\n",
      "    job = lambda: worker.work()\n",
      "  File \"<ipython-input-203-3ef2f5ae018a>\", line 46, in work\n",
      "    if len(GLOABL_REWARD_R) == 0:\n",
      "NameError: name 'GLOABL_REWARD_R' is not defined\n",
      "\n",
      "Exception in thread Thread-1060:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/yzk/.conda/envs/py36/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/yzk/.conda/envs/py36/lib/python3.6/threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-204-3a4fe299d869>\", line 19, in <lambda>\n",
      "    job = lambda: worker.work()\n",
      "  File \"<ipython-input-203-3ef2f5ae018a>\", line 46, in work\n",
      "    if len(GLOABL_REWARD_R) == 0:\n",
      "NameError: name 'GLOABL_REWARD_R' is not defined\n",
      "\n",
      "Exception in thread Thread-1063:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/yzk/.conda/envs/py36/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/yzk/.conda/envs/py36/lib/python3.6/threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-204-3a4fe299d869>\", line 19, in <lambda>\n",
      "    job = lambda: worker.work()\n",
      "  File \"<ipython-input-203-3ef2f5ae018a>\", line 46, in work\n",
      "    if len(GLOABL_REWARD_R) == 0:\n",
      "NameError: name 'GLOABL_REWARD_R' is not defined\n",
      "\n",
      "Exception in thread Thread-1059:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/yzk/.conda/envs/py36/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/yzk/.conda/envs/py36/lib/python3.6/threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-204-3a4fe299d869>\", line 19, in <lambda>\n",
      "    job = lambda: worker.work()\n",
      "  File \"<ipython-input-203-3ef2f5ae018a>\", line 46, in work\n",
      "    if len(GLOABL_REWARD_R) == 0:\n",
      "NameError: name 'GLOABL_REWARD_R' is not defined\n",
      "\n",
      "Exception in thread Thread-1074:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/yzk/.conda/envs/py36/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/yzk/.conda/envs/py36/lib/python3.6/threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-204-3a4fe299d869>\", line 19, in <lambda>\n",
      "    job = lambda: worker.work()\n",
      "  File \"<ipython-input-203-3ef2f5ae018a>\", line 46, in work\n",
      "    if len(GLOABL_REWARD_R) == 0:\n",
      "NameError: name 'GLOABL_REWARD_R' is not defined\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "import threading\n",
    "LR_A = 0.0001\n",
    "LR_C = 0.001\n",
    "N_WORKERS = multiprocessing.cpu_count()\n",
    "with tf.device(\"/cpu:0\"):\n",
    "    OPT_A = tf.keras.optimizers.RMSprop(LR_A)\n",
    "    OPT_C = tf.keras.optimizers.RMSprop(LR_C)\n",
    "    GLOBAL_AC = ACNet()  # we only need its params\n",
    "    workers = []\n",
    "    # Create worker\n",
    "    for i in range(N_WORKERS):\n",
    "        i_name = 'W_%i' % i   # worker name\n",
    "        workers.append(Worker(i_name, GLOBAL_AC))\n",
    "COORD = tf.train.Coordinator()\n",
    "\n",
    "worker_threads = []\n",
    "for worker in workers:\n",
    "    job = lambda: worker.work()\n",
    "    t = threading.Thread(target=job)\n",
    "    t.start()\n",
    "    worker_threads.append(t)\n",
    "COORD.join(worker_threads)\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.plot(np.arange(len(GLOABL_REWARD_R)), GLOABL_REWARD_R)\n",
    "# plt.xlabel('step')\n",
    "# plt.ylabel('Total moving reward')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow_core._api.v2.train' has no attribute 'RMSPropOptimizer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-125-eebba72b4b7f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRMSPropOptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow_core._api.v2.train' has no attribute 'RMSPropOptimizer'"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36_tf2",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
