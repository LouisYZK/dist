{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 <_MainThread(MainThread, started 10772)>\n",
      "4 <_MainThread(MainThread, started 10772)>\n",
      "6 <_MainThread(MainThread, started 10772)>\n",
      "8 <_MainThread(MainThread, started 10772)>\n",
      "108 <_MainThread(MainThread, started 10772)>\n",
      "208 <_MainThread(MainThread, started 10772)>\n",
      "308 <_MainThread(MainThread, started 10772)>\n",
      "408 <_MainThread(MainThread, started 10772)>\n",
      "508 <_MainThread(MainThread, started 10772)>\n"
     ]
    }
   ],
   "source": [
    "import threading \n",
    "class A(threading.Thread):\n",
    "    x = 0\n",
    "    def __init__(self):\n",
    "        threading.Thread.__init__(self)\n",
    "    def add(self,n):\n",
    "        global x\n",
    "        x = x + n\n",
    "#         x = x - n\n",
    "        print(x,threading.currentThread())\n",
    "class B(threading.Thread):\n",
    "    x = 0\n",
    "    def __init__(self):\n",
    "        threading.Thread.__init__(self)\n",
    "    def add(self,n):\n",
    "        global x\n",
    "        x = x + n**2\n",
    "#         x = x - n**2\n",
    "        print(x,threading.currentThread())\n",
    "AA = [A() for i in range(4)]\n",
    "BB = [B() for j in range(5)]\n",
    "for a in AA:\n",
    "    a.start()\n",
    "    a.add(2)\n",
    "for b in BB:\n",
    "    b.start()\n",
    "    b.add(10)\n",
    "\n",
    "import time\n",
    "time.sleep(0.1)\n",
    "\n",
    "# for a in AA:\n",
    "#     a.stop()\n",
    "# for i in range(10):\n",
    "#     for a in AA:\n",
    "#         a.join()\n",
    "#     # for b in BB:\n",
    "#     #     b.stop()\n",
    "#     for b in BB:\n",
    "#         b.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import threading\n",
    "import numpy as np\n",
    "a = np.array([[1,2,3],[3,4,5]])\n",
    "a = np.vstack((a,[3,3,3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StateProcessor():\n",
    "    \"\"\"\n",
    "    Processes a raw Atari iamges. Resizes it and converts it to grayscale.\n",
    "    图片的预处理。\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # Build the Tensorflow graph\n",
    "        with tf.variable_scope(\"state_processor\"):\n",
    "            self.input_state = tf.placeholder(shape=[210, 160, 3], dtype=tf.uint8)\n",
    "            self.output = tf.image.rgb_to_grayscale(self.input_state)\n",
    "            self.output = tf.image.crop_to_bounding_box(self.output, 34, 0, 160, 160)\n",
    "            self.output = tf.image.resize_images(\n",
    "                self.output, [84, 84], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "            self.output = tf.squeeze(self.output)\n",
    "\n",
    "    def process(self, sess, state):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            sess: A Tensorflow session object\n",
    "            state: A [210, 160, 3] Atari RGB State\n",
    "\n",
    "        Returns:\n",
    "            A processed [84, 84, 1] state representing grayscale values.\n",
    "        \"\"\"\n",
    "        return sess.run(self.output, { self.input_state: state })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Estimator\n",
    "网络定义和Value、Policy的估计方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf \n",
    "def build_shared_network(X):\n",
    "    conv1 = tf.layers.conv2d(X,filters = 16,kernal_size = 8,strides =4,\n",
    "                             activation = tf.nn.relue,name = 'conv1')\n",
    "    conv2 = tf.layers.conv2d(conv1,filters = 32, kernal_size = 4, strides = 2,\n",
    "                            activation = tf.nn.relu, name = 'conv2')\n",
    "    fc = tf.layers.dense(tf.layers.flatten(conv2),units = 256,activation = tf.nn.relu,name = 'fc')\n",
    "    return fcs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy_Estimator():\n",
    "    def __init__(self,num_outputs, reuse = False, trainable = True):\n",
    "        \"\"\"\n",
    "            num_outputs : 输出单元数量\n",
    "            resuse : 是否重使用当前存在的共享网络\n",
    "            trainable : 不需要更新lochal model的Actor将其设置为False\n",
    "        \"\"\"\n",
    "        self.num_outputs = num_outputs\n",
    "        self.states = tf.placeholder(tf.uint8 ,shape = [None,84,84,4], name = 'X')\n",
    "        self.targets = tf.placeholder(tf.float32,shape = [None],name = 'y') \n",
    "        self.actions = tf.placeholder(tf.int32,shape = [None], name = 'actions')\n",
    "        \n",
    "#         Normalize\n",
    "        X = tf.to_float(self.states) / 255.0\n",
    "        batch_szie  = tf.shape(self.states)[0]\n",
    "#         build graph\n",
    "        with tf.variable_scope('shared',reuse = reuse):\n",
    "            fc1 = build_shared_network(X)\n",
    "        with tf.variable_scope('policy_network'):\n",
    "            self.logit = tf.layers.dense(inputs = fc1, units = num_outputs,activation = None)\n",
    "            self.prob = tf.nn.softmax(self.logit) + 1e-8\n",
    "        self.predict = {\n",
    "            'logit':self.logit,\n",
    "            'prob': self.prob\n",
    "        }\n",
    "#         add entropy\n",
    "        self.entropy = tf.reduce_sum(self.logit * tf.log(self.logit),axis = 1,name = 'entropy')\n",
    "        self.entropy_mean = tf.reduce_mean(self.entropy,name = 'entropy_mean')\n",
    "        \n",
    "        # Get the predictions for the chosen actions only\n",
    "        gather_indices = tf.range(batch_size) * tf.shape(self.probs)[1] + self.actions\n",
    "        self.picked_action_probs = tf.gather(tf.reshape(self.probs, [-1]), gather_indices)\n",
    "        \n",
    "        self.losses = -(tf.log(self.picked_action_probs)*self.targets + 0.01*self.entropy)\n",
    "        self.loss = tf.reduce_sum(self.losses)\n",
    "        \n",
    "        if trainable:\n",
    "            self.optimizer = tf.train.RMSPropOptimizer(0.00025,0.99, 0.0, 1e-6)\n",
    "#             self.train_op = self.optimizer.minimize(self.loss)\n",
    "            self.grads_and_vals = self.optimizer.compute_gradients(self.loss)\n",
    "            self.grads_and_vals = [[grad,val] for grad ,val in self.grads_and_vals if grad is not None]\n",
    "            self.train_op = self.optimizer.apply_gradients(self.grads_and_vars,\n",
    "                                                           global_step=tf.train.get_global_step())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Value_Estimator():\n",
    "    def __init__(self,resuse = False,trainable = True):\n",
    "        self.states = tf.placeholder(tf.uint8 ,shape = [None,84,84,4], name = 'X')\n",
    "        self.targets = tf.placeholder(tf.float32,shape = [None],name = 'y') \n",
    "        \n",
    "        X = tf.to_float(self.states) / 255.0\n",
    "        batch_szie  = tf.shape(self.states)[0]\n",
    "        \n",
    "        with tf.variable_scope('shared',reuse = reuse):\n",
    "            fc1 = build_shared_network(X)\n",
    "        with tf.variable_scope('Value_network'):\n",
    "            self.logit = tf.layers.dense(fc1,1,activation = None)\n",
    "            self.logit = tf.squeeze(self.logit,squeeze_dim = [1],name = 'logit')\n",
    "            self.losses = tf.squared_difference(self.logit,self.targets)\n",
    "            self.loss = tf.reduce_sum(self.losses,name = 'loss')\n",
    "            self.predictions = {\n",
    "                'logit':self.logit\n",
    "            }\n",
    "        if trainable :\n",
    "            self.optimizer = tf.train.RMSPropOptimizer(0.00025,0.99, 0.0, 1e-6)\n",
    "            self.grads_and_vals = self.optimizer.compute_gradients(self.loss)\n",
    "            self.grads_and_vals = [[grad,val] for grad ,val in self.grads_and_vals if grad is not None]\n",
    "            self.train_op = self.optimizer.apply_gradients(self.grads_and_vars,\n",
    "                                                           global_step=tf.train.get_global_step())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import itertools\n",
    "Transition = collections.namedtuple(\"Transition\", [\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "def make_copy_variables_op(v1_list,v2_list):\n",
    "    \"\"\"\n",
    "        这是一个交换拷贝Variable类型的函数,返回这些操作（op）的集合\n",
    "    \"\"\"\n",
    "    v1_list = list(sorted(v1_list,key = lambda v:v.name))\n",
    "    v2_list = list(sorted(v2_list,key = lambda v:v.name))\n",
    "    update_op = []\n",
    "    for v1,v2 in zip(v1_list,v2_list):\n",
    "        op = v2.assign(v1)\n",
    "        update_op.append(op)\n",
    "    return update_op\n",
    "def make_train_op(local_estimator, global_estimator):\n",
    "    \"\"\"\n",
    "        create an operation that applies local estimator's gradient to global estimator\n",
    "    \"\"\"\n",
    "    local_grad ,_ = zip(*local_estimator.grads_and_vals)\n",
    "    local_grad = tf.clip_by_global_norm(local_grad,5.0) # 调整输入的2范式值不能超过某值\n",
    "    _, global_var = zip(*global_estimator.grads_and_vals)\n",
    "    local_grad_and_global_var = list(zip(local_grad,global_var))\n",
    "    return global_estimator.optimizer.apply_gradients(local_global_grads_and_vars,\n",
    "          global_step=tf.train.get_global_step())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Worker():\n",
    "    \"\"\"\n",
    "        An A3C worker run episode locally and update global shared policy and value network\n",
    "    \"\"\"\n",
    "    def __init__(self,name,env,policy_net,value_net,global_counter,reward_discount=0.99,max_global_steps =None):\n",
    "        \"\"\"\n",
    "            name : 每个worker的命名\n",
    "            policy_net : instance of global shared policy net\n",
    "            global_count :  Iterator that holds the global step\n",
    "            max_global_steps : If set, stop coordinator when global_counter > max_global_steps\n",
    "        \"\"\"\n",
    "        self.name = name\n",
    "        self.env = env\n",
    "        self.global_policy_net = policy_net\n",
    "        self.global_value_net = value_net\n",
    "        self.global_counter = global_counter\n",
    "        self.global_step = tf.train.get_global_step()\n",
    "        self.local_counter = itertools.count()\n",
    "        self.sp = StateProcessor()\n",
    "        \n",
    "#         创建不异步更新的net(local)\n",
    "        with tf.variable_scope(name):\n",
    "            self.policy_net = Policy_Estimator(policy_net.num_outputs)\n",
    "            self.value_net = Value_Estimator(reuse = True)\n",
    "        # Op to copy params from global policy/valuenets\n",
    "#         将全局参数给local\n",
    "        self.copy_params_op = make_copy_params_op(\n",
    "          tf.contrib.slim.get_variables(scope=\"global\", collection=tf.GraphKeys.TRAINABLE_VARIABLES),\n",
    "          tf.contrib.slim.get_variables(scope=self.name+'/', collection=tf.GraphKeys.TRAINABLE_VARIABLES))\n",
    "        \n",
    "        self.pnet_train_op = make_train_op(self.policy_net,self.global_policy_net)\n",
    "        self.vnet_train_op = make_train_op(self.value_net,self.global_value_net)\n",
    "        \n",
    "        self.state = None\n",
    "    def run():\n",
    "        with sess.as_default(), sess.graph.as_default():\n",
    "      # Initial state\n",
    "          self.state = atari_helpers.atari_make_initial_state(self.sp.process(self.env.reset()))\n",
    "        try:\n",
    "            while not coord.should_stop():\n",
    "          # Copy Parameters from the global networks\n",
    "            sess.run(self.copy_params_op)\n",
    "\n",
    "          # Collect some experience\n",
    "            transitions, local_t, global_t = self.run_n_steps(t_max, sess)\n",
    "            \n",
    "            if self.max_global_steps is not None and global_t >= self.max_global_steps:\n",
    "                tf.logging.info(\"Reached global step {}. Stopping.\".format(global_t))\n",
    "                coord.request_stop()\n",
    "            return\n",
    "\n",
    "          # Update the global networks\n",
    "            self.update(transitions, sess)\n",
    "        except tf.errors.CancelledError:\n",
    "        return\n",
    "    \n",
    "    def _policy_net_predict(self, state, sess):\n",
    "        feed_dict = { self.policy_net.states: [state] }\n",
    "        preds = sess.run(self.policy_net.predictions, feed_dict)\n",
    "        return preds[\"probs\"][0]\n",
    "\n",
    "    def _value_net_predict(self, state, sess):\n",
    "        feed_dict = { self.value_net.states: [state] }\n",
    "        preds = sess.run(self.value_net.predictions, feed_dict)\n",
    "        return preds[\"logits\"][0]\n",
    "    def run_n_steps(self,n,sess):\n",
    "        transitions = []\n",
    "        for _ in range(n):\n",
    "            action_probs = self._policy_net_predict(self.state,sess)\n",
    "            action = np.random.choice(np.arange(len(action_probs)),p = action_probs)\n",
    "            next_state, reward,done, _ = self.env.step(action)\n",
    "            next_state = self.sp.process(next_state)\n",
    "            trainsitions.append(Transition(state = self.state,action = self.action,\n",
    "                                           reward = reward,next_state = next_state,done = done))\n",
    "            local_t = next(self.local_counter)\n",
    "            global_t = next(self.global_counter)\n",
    "        if local_t % 100 == 0:\n",
    "            tf.logging.info(\"{}: local Step {}, global step {}\".format(self.name, local_t, global_t))\n",
    "\n",
    "        if done:\n",
    "            self.state = atari_helpers.atari_make_initial_state(self.sp.process(self.env.reset()))\n",
    "            break\n",
    "        else:\n",
    "            self.state = next_state\n",
    "    def update(transition,sess):\n",
    "        \"\"\"\n",
    "            update global policy / value network\n",
    "        \"\"\"\n",
    "            reward = 0.0\n",
    "        if not transitions[-1].done:\n",
    "            reward = self._value_net_predict(transitions[-1].next_state, sess)\n",
    "\n",
    "    # Accumulate minibatch exmaples\n",
    "        states = []\n",
    "        policy_targets = []\n",
    "        value_targets = []\n",
    "        actions = []\n",
    "        \n",
    "        for transition in transitions[::-1]:\n",
    "            reward = transition.reward + self.discount_factor * reward\n",
    "            policy_target = (reward - self._value_net_predict(transition.state, sess))\n",
    "      # Accumulate updates\n",
    "            states.append(transition.state)\n",
    "            actions.append(transition.action)\n",
    "            policy_targets.append(policy_target)\n",
    "            value_targets.append(reward)\n",
    "        feed_dict = {\n",
    "            self.policy_net.states: np.array(states),\n",
    "            self.policy_net.targets: policy_targets,\n",
    "            self.policy_net.actions: actions,\n",
    "            self.value_net.states: np.array(states),\n",
    "            self.value_net.targets: value_targets,\n",
    "        }\n",
    "        global_step, pnet_loss, vnet_loss, _, _ = sess.run([\n",
    "            self.global_step,\n",
    "            self.policy_net.loss,\n",
    "            self.value_net.loss,\n",
    "            self.pnet_train_op,\n",
    "            self.vnet_train_op,\n",
    "        ], feed_dict)\n",
    "        return pnet_loss, vnet_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train(Brain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import tensorflow as tf\n",
    "NUM_WORKERS = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.device('/cpu:0'):\n",
    "    global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "#     global policy and value nets:\n",
    "    with tf.variable_scope('global') as vs:\n",
    "        policy_net = Policy_Estimator(num_outputs = NUM_OUTPUTS)\n",
    "        value_net = Value_Estimator(reuse = True)\n",
    "    global_counter = itertools.count()\n",
    "    \n",
    "    workers = []\n",
    "    for worker_id in range(NUM_WORKERS):\n",
    "        worker = Worker(\n",
    "            name = 'Worker_{}'.format(worker_id),\n",
    "            env = make_env(),\n",
    "            policy_net = policy_net,\n",
    "            value_net = value_net,\n",
    "            global_counter = global_counter,\n",
    "            discount_factor = 0.99,\n",
    "        )\n",
    "        workers.append(worker)\n",
    "    saver = tf.train.Saver(keep_checkpoint_every_n_hours=2.0, max_to_keep=10)\n",
    "    with tf.Session() as tf:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        coord = tf.train.Coordinator()\n",
    "        # Load a previous checkpoint if it exists\n",
    "        latest_checkpoint = tf.train.latest_checkpoint(CHECKPOINT_DIR)\n",
    "        worker_threads = []\n",
    "        for worker in wokers:\n",
    "            worker_fn = lambda worker=worker :worker.run(sess,coord,FlAGS.t_max)\n",
    "            t = threading.Thread(target = worker_fn)\n",
    "            t.start()\n",
    "            worker_threads.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
