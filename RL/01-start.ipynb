{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-learning Simple instances\n",
    "Initial Q-table arbitary\n",
    "\n",
    "> - Repeat(for each episode)\n",
    "    - inital **s**    \n",
    "    - Repeat(for each step in episode)   \n",
    "        - choose **a** from the policy Q(GREEDY-RATE $\\epsilon$)        \n",
    "        - Take action **a** get r, s'        \n",
    "        - Update $Q(s,a) \\leftarrow Q(s,a) + \\alpha[r+(\\gamma max_{a'}Q(s,a') - Q(s,a))]$\n",
    "        - $s \\leftarrow s'$\n",
    "    - Until s is terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   left  right\n",
      "0   0.0    0.0\n",
      "1   0.0    0.0\n",
      "2   0.0    0.0\n",
      "3   0.0    0.0\n",
      "4   0.0    0.0\n",
      "5   0.0    0.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "ACTIONS = ['left','right']\n",
    "STATE = 6\n",
    "GREEDY_RATE = 0.9 # trade off between exploration and exploitation\n",
    "ALPHA = 0.1 # learning rate\n",
    "GAMMA = 0.9 # discount rate\n",
    "MAX_EPISODES = 13 # max episodes\n",
    "FRESH_TIME = 0.01 # update time \n",
    "np.random.seed(100)\n",
    "def build_q_table(state,actions):\n",
    "    table = pd.DataFrame(data = np.zeros((state,len(actions))),columns = actions)\n",
    "    return table\n",
    "print(build_q_table(STATE,ACTIONS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(state,q_table):\n",
    "    table_actions = q_table.iloc[state,:]\n",
    "    if np.random.uniform()<GREEDY_RATE or ((table_actions ==0).all()):\n",
    "        action = np.random.choice(ACTIONS)\n",
    "    else:\n",
    "        action = table_actions.idxmax()\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected character after line continuation character (<ipython-input-4-d2d052cf7f59>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-4-d2d052cf7f59>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    C:\\Users\\Administrator\\.jupyter\\custom &\u001b[0m\n\u001b[1;37m                                            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unexpected character after line continuation character\n"
     ]
    }
   ],
   "source": [
    "C:\\Users\\Administrator\\.jupyter\\custom &\n",
    "C:\\Users\\Administrator\\AppData\\Roaming\\jupyter\\nbextensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_env_feedback(S,A):\n",
    "    if A =='left':\n",
    "        if S == STATE -2:\n",
    "            S_ = 'terminal'\n",
    "            r = 1\n",
    "        else:\n",
    "            S_ = S + 1\n",
    "            r =0\n",
    "    else:\n",
    "        r = 0\n",
    "        if S ==0:\n",
    "            S_ = S\n",
    "        else:\n",
    "            S_ = S-1\n",
    "    return S_,r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_env(S, episode, step_counter):\n",
    "    # This is how environment be updated\n",
    "    env_list = ['-']*(STATE-1) + ['T']   # '---------T' our environment\n",
    "    if S == 'terminal':\n",
    "        interaction = 'Episode %s: total_steps = %s' % (episode+1, step_counter)\n",
    "        print('\\r{}'.format(interaction), end='')\n",
    "        time.sleep(2)\n",
    "        print('\\r                                ', end='')\n",
    "    else:\n",
    "        env_list[S] = 'o'\n",
    "        interaction = ''.join(env_list)\n",
    "        print('\\r{}'.format(interaction), end='')\n",
    "        time.sleep(FRESH_TIME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       left     right\n",
      "0  0.050463  0.023836\n",
      "1  0.107177  0.020782\n",
      "2  0.253266  0.022860\n",
      "3  0.487156  0.072512\n",
      "4  0.745813  0.194329\n",
      "5  0.000000  0.000000\n"
     ]
    }
   ],
   "source": [
    "q_table = build_q_table(STATE,ACTIONS)\n",
    "for episode in range(MAX_EPISODES):\n",
    "    is_terminal = False\n",
    "    S = 0\n",
    "    step_counter = 0\n",
    "    update_env(S,episode,step_counter)\n",
    "    while not is_terminal:\n",
    "        A = choose_action(S,q_table)\n",
    "        S_,r = get_env_feedback(S,A)\n",
    "        q_predict = q_table.loc[S,A]\n",
    "        if S_ !='terminal':\n",
    "            q_target = r + GAMMA*q_table.iloc[S_,:].max()\n",
    "        else:\n",
    "            q_target = r\n",
    "            is_terminal = True\n",
    "        q_table.loc[S,A] += ALPHA*(q_target - q_predict)\n",
    "        S = S_\n",
    "        update_env(S,episode,step_counter+1)\n",
    "        step_counter+=1\n",
    "print(q_table)                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
