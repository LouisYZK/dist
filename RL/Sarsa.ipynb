{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sarsa & Saras($\\lambda$)\n",
    "> - Repeat(for each episode)\n",
    "    - inital **s**    \n",
    "    - Repeat(for each step in episode)   \n",
    "        - choose **a** from **s** based on the policy Q(GREEDY-RATE $\\epsilon$)        \n",
    "        - Take action **a** get r, s'\n",
    "        - *Choose action **a'** from **s'** based on the ploicy Q(GREEDY-RATE $\\epsilon$) *\n",
    "        - *Update $Q(s,a) \\leftarrow Q(s,a) + \\alpha[r+(\\gamma Q(s',a') - Q(s,a))]$*\n",
    "        - $s \\leftarrow s'$\n",
    "        - $a \\leftarrow a'$\n",
    "    - Until s is terminal\n",
    "    \n",
    "Sarsa 与普通Q-learning的区别是Q表的更新方式和动作选择上。\n",
    "\n",
    "Q-learning得到下一个action和State的估计后会预测最大的值$max_{a'}Q(s',a')$来更新Q表，比较贪婪，在动作选择时却不一定选择$a' = argmax_{a'}Q(s',a')$ ；\n",
    "\n",
    "而Sarsa的直接选择预测出的在做出a之后直接选择出下一阶段的a',并且执行，并根据选择的a'进行预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 物理环境env\n",
    "import numpy as np\n",
    "import time\n",
    "import sys\n",
    "if sys.version_info.major == 2:\n",
    "    import Tkinter as tk\n",
    "else:\n",
    "    import tkinter as tk\n",
    "\n",
    "\n",
    "UNIT = 40   # pixels\n",
    "MAZE_H = 4  # grid height\n",
    "MAZE_W = 4  # grid width\n",
    "\n",
    "\n",
    "class Maze(tk.Tk, object):\n",
    "    def __init__(self):\n",
    "        super(Maze, self).__init__()\n",
    "        self.action_space = ['u', 'd', 'l', 'r']\n",
    "        self.n_actions = len(self.action_space)\n",
    "        self.title('maze')\n",
    "        self.geometry('{0}x{1}'.format(MAZE_H * UNIT, MAZE_H * UNIT))\n",
    "        self._build_maze()\n",
    "\n",
    "    def _build_maze(self):\n",
    "        self.canvas = tk.Canvas(self, bg='white',\n",
    "                           height=MAZE_H * UNIT,\n",
    "                           width=MAZE_W * UNIT)\n",
    "\n",
    "        # create grids\n",
    "        for c in range(0, MAZE_W * UNIT, UNIT):\n",
    "            x0, y0, x1, y1 = c, 0, c, MAZE_H * UNIT\n",
    "            self.canvas.create_line(x0, y0, x1, y1)\n",
    "        for r in range(0, MAZE_H * UNIT, UNIT):\n",
    "            x0, y0, x1, y1 = 0, r, MAZE_H * UNIT, r\n",
    "            self.canvas.create_line(x0, y0, x1, y1)\n",
    "\n",
    "        # create origin\n",
    "        origin = np.array([20, 20])\n",
    "\n",
    "        # hell\n",
    "        hell1_center = origin + np.array([UNIT * 2, UNIT])\n",
    "        self.hell1 = self.canvas.create_rectangle(\n",
    "            hell1_center[0] - 15, hell1_center[1] - 15,\n",
    "            hell1_center[0] + 15, hell1_center[1] + 15,\n",
    "            fill='black')\n",
    "        # hell\n",
    "        hell2_center = origin + np.array([UNIT, UNIT * 2])\n",
    "        self.hell2 = self.canvas.create_rectangle(\n",
    "            hell2_center[0] - 15, hell2_center[1] - 15,\n",
    "            hell2_center[0] + 15, hell2_center[1] + 15,\n",
    "            fill='black')\n",
    "\n",
    "        # create oval\n",
    "        oval_center = origin + UNIT * 2\n",
    "        self.oval = self.canvas.create_oval(\n",
    "            oval_center[0] - 15, oval_center[1] - 15,\n",
    "            oval_center[0] + 15, oval_center[1] + 15,\n",
    "            fill='yellow')\n",
    "\n",
    "        # create red rect\n",
    "        self.rect = self.canvas.create_rectangle(\n",
    "            origin[0] - 15, origin[1] - 15,\n",
    "            origin[0] + 15, origin[1] + 15,\n",
    "            fill='red')\n",
    "\n",
    "        # pack all\n",
    "        self.canvas.pack()\n",
    "\n",
    "    def reset(self):\n",
    "        self.update()\n",
    "        time.sleep(0.5)\n",
    "        self.canvas.delete(self.rect)\n",
    "        origin = np.array([20, 20])\n",
    "        self.rect = self.canvas.create_rectangle(\n",
    "            origin[0] - 15, origin[1] - 15,\n",
    "            origin[0] + 15, origin[1] + 15,\n",
    "            fill='red')\n",
    "        # return observation\n",
    "        return self.canvas.coords(self.rect)\n",
    "\n",
    "    def step(self, action):\n",
    "        s = self.canvas.coords(self.rect)\n",
    "        base_action = np.array([0, 0])\n",
    "        if action == 0:   # up\n",
    "            if s[1] > UNIT:\n",
    "                base_action[1] -= UNIT\n",
    "        elif action == 1:   # down\n",
    "            if s[1] < (MAZE_H - 1) * UNIT:\n",
    "                base_action[1] += UNIT\n",
    "        elif action == 2:   # right\n",
    "            if s[0] < (MAZE_W - 1) * UNIT:\n",
    "                base_action[0] += UNIT\n",
    "        elif action == 3:   # left\n",
    "            if s[0] > UNIT:\n",
    "                base_action[0] -= UNIT\n",
    "\n",
    "        self.canvas.move(self.rect, base_action[0], base_action[1])  # move agent\n",
    "\n",
    "        s_ = self.canvas.coords(self.rect)  # next state\n",
    "\n",
    "        # reward function\n",
    "        if s_ == self.canvas.coords(self.oval):\n",
    "            reward = 1\n",
    "            done = True\n",
    "            s_ = 'terminal'\n",
    "        elif s_ in [self.canvas.coords(self.hell1), self.canvas.coords(self.hell2)]:\n",
    "            reward = -1\n",
    "            done = True\n",
    "            s_ = 'terminal'\n",
    "        else:\n",
    "            reward = 0\n",
    "            done = False\n",
    "\n",
    "        return s_, reward, done\n",
    "\n",
    "    def render(self):\n",
    "        time.sleep(0.1)\n",
    "        self.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'u'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 思维决策\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "class RL(object):\n",
    "    def __init__(self,action_span,learning_rate = 0.9,reward_decay=0.1,e_greedy=0.9):\n",
    "        self.actions = action_span\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = reward_decay\n",
    "        self.epsilon = e_greedy\n",
    "        self.q_table = pd.DataFrame(columns = actions)\n",
    "    def check_state_exist(self,observation):\n",
    "        if observation not in self.q_table.index:\n",
    "            self.q_table = self.q_table.append(\n",
    "                pd.Series(index = self.q_table.columns,data = [0]*len(actions),name = observation)\n",
    "            )\n",
    "    def choose_action(self,observation):\n",
    "        self.check_state_exist(observation)\n",
    "        action_table = self.q_table.loc[observation,:]\n",
    "        if np.random.uniform() < self.epsilon:\n",
    "            action_table.reindex(np.random.permutation(action_table.index))\n",
    "            action_chosen = action_table.idxmax()\n",
    "        else:\n",
    "            action_chosen = np.random.choice(actions)\n",
    "        return action_chosen\n",
    "    def learn(self):\n",
    "        pass\n",
    "rl = RL(['u','d','l','r'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "super() argument 1 must be type, not Sarsa_table",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-53-f1f483eaec5b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mSarsa_table\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mRL\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maction_span\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mreward_decay\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.9\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0me_greedy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.9\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSarsa_table\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction_span\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mreward_decay\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0me_greedy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mS\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mS_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ma_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_state_exist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mS_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-45-bf94e6758963>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, action_span, learning_rate, reward_decay, e_greedy)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mSarsa_table\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mRL\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maction_span\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mreward_decay\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.9\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0me_greedy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.9\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSarsa_table\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction_span\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mreward_decay\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0me_greedy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mS\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mS_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ma_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_state_exist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mS_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: super() argument 1 must be type, not Sarsa_table"
     ]
    }
   ],
   "source": [
    "class Sarsa_table(RL):\n",
    "    def __init__(self,action_span,learning_rate=0.1,reward_decay = 0.9,e_greedy = 0.9):\n",
    "        super(Sarsa_table,self).__init__(action_span,learning_rate,reward_decay,e_greedy)\n",
    "    def learn(self,S,a,r,S_,a_):\n",
    "        self.check_state_exist(S_)\n",
    "        q_predict = self.q_table.loc[S,a]\n",
    "        if S_ !='terminal':\n",
    "            q_target = r + self.gamma*self.q_table.loc[S_,a_]\n",
    "        else:\n",
    "            q_target = r\n",
    "        self.q_table.loc.loc[S,a] += self.lr*(q_target - q_predict)         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 主程序\n",
    "def update():\n",
    "    for episode in range(100):\n",
    "        # initial observation\n",
    "        observation = env.reset()\n",
    "\n",
    "        # RL choose action based on observation\n",
    "        action = RL.choose_action(str(observation))\n",
    "\n",
    "        while True:\n",
    "            # fresh env\n",
    "            env.render()\n",
    "\n",
    "            # RL take action and get next observation and reward\n",
    "            observation_, reward, done = env.step(action)\n",
    "\n",
    "            # RL choose action based on next observation\n",
    "            action_ = RL.choose_action(str(observation_))\n",
    "\n",
    "            # RL learn from this transition (s, a, r, s, a) ==> Sarsa\n",
    "            RL.learn(str(observation), action, reward, str(observation_), action_)\n",
    "\n",
    "            # swap observation and action\n",
    "            observation = observation_\n",
    "            action = action_\n",
    "\n",
    "            # break while loop when end of this episode\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "    # end of game\n",
    "    print('game over')\n",
    "    env.destroy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Sarsa_table' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-54-f02cde161872>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMaze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mRL\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSarsa_table\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction_span\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_actions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mafter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mupdate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmainloop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'Sarsa_table' object is not callable"
     ]
    }
   ],
   "source": [
    "env = Maze()\n",
    "RL = Sarsa_table(action_span=list(range(env.n_actions)))\n",
    "env.after(100, update)\n",
    "env.mainloop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sarsa($\\lambda$)\n",
    "与Sarsa(0)方法相比，有权重地进行单元更新。\n",
    "\n",
    "![](https://morvanzhou.github.io/static/results/reinforcement-learning/3-3-1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Sarsa_lambda_table(RL):\n",
    "    def __init__(self,action_span,learning_rate=0.1,reward_decay = 0.9,e_greedy = 0.9,trace_decay = 0.9):\n",
    "        super().__init__(action_span,learning_rate,reward_decay,e_greedy)\n",
    "        self.lambda = trace_decay\n",
    "        self.eligibility_table  = self.q_table.copy() #即算法步骤中的E(S,A)\n",
    "    def check_state_exist(self,state):\n",
    "        if state not in self.q_table.index:\n",
    "            add_ = pd.Series(data = [0]*len(self.actions),index = self.q_table.clumns,name = state)\n",
    "            self.q_table =  self.q_table.append(add_)\n",
    "            self.eligibility_table =  self.eligibility_table.append(add_)\n",
    "    def learn(self,S,a,r,S_,a_):\n",
    "        self.check_state_exist(S_)\n",
    "        q_predict = self.q_table.loc[S,a]\n",
    "        if S_ != 'terminal':\n",
    "            q_target = r + self.gamma*self.q_table.loc[S_,a_]\n",
    "        else:\n",
    "            q_target= r\n",
    "        error = q_target - q_predict\n",
    "        self.eligibility_table.loc[S,:] = 0\n",
    "        self.eligibility_table.loc[S,a] = 1\n",
    "        self.q_table += self.lr * self.eligibility_table * error\n",
    "        self.eligibility_table *= self.lambda"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
