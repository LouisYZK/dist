{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Decision Process and Dynamic Programming\n",
    "- Date: Match 2019\n",
    "- Material from Reinforcement Learning:An Introduction,2nd,Rechard.S.Sutton; \n",
    "- Code from [dennyBritze]('https://github.com/dennybritz/reinforcement-learning/tree/master/DP'), 部分做了修改；\n",
    "\n",
    "**content**\n",
    "- MDP problems setup\n",
    "- Bellman Equation\n",
    "- Optimal Policies and Optimal Value Functions\n",
    "- Model-Based: Dynamic Programming\n",
    "- Policy Evaluation\n",
    "- Policy Improvement\n",
    "- Policy Iteration\n",
    "- Value Iteration\n",
    "- Generalized Policy Iteration(GPI)\n",
    "\n",
    "**Abstract**\n",
    "\n",
    "MDP过程是RL环境中常见的范式，DP是解决有限MDP问题的可最优收敛办法，效率在有效平方级。DP算法基本思想是基于贝尔曼方程进行Bootstrapping，即用估计来学习估计（learn a guess from a guess)。DP需要经过反复的**策略评估**和**策略提升**过程，最终收敛到最优的策略和值函数。这一过程其实是RL很多算法的基本过程，即先进行评估策略（Prediction）再优化策略。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MDP problems set up\n",
    "在**RL problems set up**中我们知道RL基本要素是Agent和Enviornment, 环境的种类很多，但大多都可以抽象成一个马尔科夫决策过程（MDP）或者部分马尔科夫决策过程(POMDP);\n",
    "\n",
    "MDPs are a mathematically idealized form of the reinforcement learning problem for which precise\n",
    "theoretical statements can be made.\n",
    "\n",
    "**Key elements** of MDP：$<\\mathcal{S}, \\mathcal{A}, \\mathcal{P}, \\mathcal{R},\\gamma>$\n",
    "\n",
    "| 名称 | 表达式|\n",
    "|:--------------------:| :----------------------------------------------------------------- |\n",
    "|状态转移矩阵（一个Markov Matrix）| $P_{ss'}^a=P(S_{t+1}=s'|S_t=s,A_t=a)$|  \n",
    "|奖励函数 | $R_{s}^a=\\mathbb{E}_{\\pi}[R_{t+1}|S_t=s,A_t=a]$| \n",
    "|累计奖励 | $G_t=\\sum_{k=0}^\\infty\\gamma^k R_{t+1+k}$ |\n",
    "|值函数（Value Function）| $V_\\pi(a)=\\mathbb{E}[G_t|S_t=s]$ |\n",
    "|动作值函数（Action Value Fucntion）| $Q_\\pi(s,a)=\\mathbb{E}[G_t|S_t=s,A_t=a]$|\n",
    "|策略（Policy）| $\\pi(a|s)=\\mathbb{P}(A_t=a|S_t=s)$ |\n",
    "|奖励转移方程 | $R_{t+1}=R_{t+1}(S_t,A_t,S_{t+1})$|\n",
    "|某策略下的状态转移方程 | $P_{ss'}^\\pi=\\mathbb{P}(S_{t+1}=s'|S_t=s)=\\sum_{a}\\pi(a|s)P_{ss'}^a$ |\n",
    "|某状态某策略下的奖励函数 | $R_{s}^\\pi=\\sum_{a}\\pi(a|s)R_{s}^a$|\n",
    "\n",
    "## Bellman Equation\n",
    "贝尔曼方程将某时刻的值函数与其下一时刻的值函数联系起来：\n",
    "$$G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + ...$$\n",
    "$$G_t = \\sum_{k=t+1}^{T}\\gamma^{k-t-1}R_k = R_{t+1} + \\gamma G_{t+1}$$\n",
    "对于**动作-值函数来说**：\n",
    "$$\n",
    "\\begin{eqnarray}\\nonumber\n",
    "    Q_\\pi(s,a)&=&\\mathbb{E}_{\\pi}[G_t|S_t=s,A_t=a]\\\\\\nonumber\n",
    "    &=&\\mathbb{E}_{\\pi}[R_{t+1}+\\gamma V_\\pi(S_{t+1})|S_t=s,A_t=a]\\\\\\label{BEQ1}\n",
    "    &=&R_{s}^a+\\gamma\\sum_{s'}P_{ss'}^aV_\\pi(s')\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "对于**值函数来说**\n",
    "$$\n",
    "\\begin{eqnarray}\\nonumber\n",
    "    V_\\pi(s)&=&\\mathbb{E}_{\\pi}[G_t|S_t=s]\\\\\\nonumber\n",
    "    &=&\\sum_a\\pi(a|s)Q_\\pi(s,a)\\\\\\nonumber\n",
    "    &=&\\sum_a\\pi(a|s)\\{R_{s}^a+\\gamma\\sum_{s'}P_{ss'}^aV_\\pi(s')\\} \\\\\\label{BEQ2}\n",
    "    &=&\\sum_a\\pi(a|s)R_{s}^\\pi+\\gamma\\sum_{s'}P_{ss'}^\\pi V_\\pi(s') \\quad (2)\\\\\\label{BEQ2_alg}\n",
    "    &=&\\sum_a\\pi(a|s)\\sum_{s'}P_{ss'}^a\\{R_{ss'}^a+\\gamma V_\\pi(s') \\}\\quad (3)\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "基于公式2可以写成矩阵的形式：$V^\\pi = R_s^\\pi + P^\\pi V^\\pi$\n",
    "## Optimal Policies and Optimal Value Functions\n",
    "最优策略和最优的值函数关系如下：\n",
    "$$v_* = \\max_\\pi V_\\pi(s)$$\n",
    "$$Q_*(s,a) = \\max_\\pi Q_pi(s,a)$$\n",
    "$$V_*(s) = \\max_a Q_*(s,a)$$\n",
    "\n",
    "Find optimal policy by:\n",
    "$$\n",
    "    \\begin{equation}\n",
    "        \\pi_{*}(a|s) = \\left\\{\\begin{array}{cc}\n",
    "        1 & \\text{if}\\ a=\\arg\\min_a Q_*(s,a) \\\\\n",
    "        0& \\text{Otherwise}\n",
    "        \\end{array}\\right..\n",
    "    \\end{equation}\n",
    "$$\n",
    "在最优策略下的贝尔曼方程为Bellman Optimality Equation:\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "    Q_*(s,a)&=&R_{s}^a+\\gamma\\sum_{s'}P_{ss'}^aV_*(s')\\\\\\label{BOEQ1}\n",
    "            &=&R_{s}^a+\\gamma\\sum_{s'}P_{ss'}^a\\max_{a'}Q_*(s',a').\n",
    "    \\end{eqnarray}\n",
    "$$\n",
    "$$\n",
    "         \\begin{eqnarray}\\label{BOEQ2}\n",
    "    V_*(s)&=&\\max_a\\{R_{s}^a+\\gamma\\sum_{s'}P_{ss'}^aV_*(s')\\}.\n",
    "    \\end{eqnarray}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic Programming\n",
    "DP算法要求MDP的全部信息完全可知，依据Bellman Optimality Equation为基本思想进行策略迭代出最优结果；\n",
    "## Policy Evaluation\n",
    "策略评估是在给定一个策略的情况下计算出Value Function的过程，迭代的更新规则是:\n",
    "$$v_{k+1}(s) = \\mathbb{E}_\\pi[R_{t+1} + \\gamma v_k(S_{t+1})|S_t = s]$$\n",
    "$$=\\sum_a \\pi(a|s)\\sum_{s'}P_{ss'}^{a}(R_{ss'}+\\gamma v_k(s'))$$\n",
    "迭代一定次数后$v_\\pi$会趋于稳定，评估结束。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Gridword\n",
    "\n",
    "用此例子来说明DP的一些算法\n",
    "![](https://pic.superbed.cn/item/5ca239e53a213b041788611a)\n",
    "\n",
    "探索出发点到终点的路径，动作空间是四个方向， 除了终点其他坐标的Reward均为-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import gym_gridworlds\n",
    "import numpy as np\n",
    "\n",
    "# 策略评估算法\n",
    "def policy_val(policy, env, GAMMA=1.0, theta=0.0001):\n",
    "    V = np.zeros(env.observation_space.n)\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in range(env.observation_space.n):\n",
    "            v = 0\n",
    "            for a, action_prob in enumerate(policy[s]):\n",
    "                for next_state, next_prob in enumerate(env.P[a,s]):\n",
    "                    reward = env.R[a, next_state]\n",
    "                    v += action_prob*next_prob*(reward + GAMMA*V[next_state])\n",
    "            delta = max(delta, V[s]-v)\n",
    "            V[s] = v\n",
    "        if delta < theta:\n",
    "            break\n",
    "    return np.array(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.        , -12.99934883, -18.99906386, -20.9989696 ,\n",
       "       -12.99934883, -16.99920093, -18.99913239, -18.99914232,\n",
       "       -18.99906386, -18.99913239, -16.9992679 , -12.9994534 ,\n",
       "       -20.9989696 , -18.99914232, -12.9994534 ])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('Gridworld-v0')\n",
    "# 初始随机策略\n",
    "random_policy = np.ones((env.observation_space.n, env.action_space.n)) / env.action_space.n\n",
    "# 评估这个随机策略得到稳定的Value Function:\n",
    "policy_val(random_policy, env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Improvement\n",
    "定义：The process of making a new policy that improves on an original policy, by making it **greedy** with\n",
    "respect to the value function of the original policy, is called policy improvement.\n",
    "\n",
    "### policy improvement theorem\n",
    "假设$\\pi$和$\\pi'$是两个确定的策略，对于所有的$s\\in \\mathcal{S}$\n",
    "如果 $$q_\\pi(s, \\pi'(s)) \\ge v_\\pi(s)$$\n",
    "那么可以证明得到：\n",
    "$$v_\\pi'(s) \\ge v_\\pi(s)$$\n",
    "所以策略提升的过程就是：$$ \\pi \\xleftarrow[]{} \\mbox{Greedy}(V_\\pi).$$\n",
    "## Policy Iteration\n",
    "Policy Iteration = Policy Evaluation + Policy Improvement\n",
    "\n",
    "给定策略--评估策略得到各个V(s)--greedy提升出新的策略--评估新策略--直到策略不发生变化\n",
    "\n",
    "![](https://pic.superbed.cn/item/5ca365d73a213b041793d638)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_step_lookahead(state, V, GAMMA):\n",
    "        A = np.zeros(env.action_space.n)\n",
    "        for a in range(env.action_space.n):\n",
    "            for next_state, prob in enumerate(env.P[a, state]):\n",
    "                reward = env.R[a, next_state]\n",
    "                A[a] += prob * (reward + GAMMA*V[next_state])\n",
    "        return A\n",
    "\n",
    "def policy_improvement(env, policy_eval_fun=policy_val, GAMMA=1.0):\n",
    "\n",
    "    # 用随机策略开始\n",
    "    policy = np.ones((env.observation_space.n, env.action_space.n)) / env.action_space.n\n",
    "\n",
    "    while True:\n",
    "        V = policy_eval_fun(policy, env, GAMMA)\n",
    "        policy_stable = True\n",
    "        for s in range(env.observation_space.n):\n",
    "            chosen_a = np.argmax(policy[s])\n",
    "            action_values = one_step_lookahead(s, V, GAMMA)\n",
    "            best_a = np.argmax(action_values)\n",
    "\n",
    "            # 贪心的方式更新策略\n",
    "            if chosen_a != best_a:\n",
    "                policy_stable = False\n",
    "            policy[s] = np.eye(env.action_space.n)[best_a]\n",
    "\n",
    "        if policy_stable:\n",
    "            return policy, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1., 0., 0., 0.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.]]),\n",
       " array([ 0.,  0., -1., -2.,  0., -1., -2., -1., -1., -2., -1.,  0., -2.,\n",
       "        -1.,  0.]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 得到稳定的策略和V(s)\n",
    "policy_improvement(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Iteration\n",
    "跟Policy Iteration的区别在于省略Policy Evaluation的过程为一步计算，这样降低了迭代次数，同时保证收敛结果依然为$v_*$，边评估边提升。\n",
    "\n",
    "省略后的Evaluation：\n",
    "$$v_{k+1}(s) = \\max_a \\sum_s' P_{ss'}^a(R_{s'}^a + \\gamma v_k(s'))$$\n",
    "而之前的评估策略迭代更多：\n",
    "$$v_{k+1}(s) = \\sum_a \\pi(a|s)\\sum_{s'}P_{ss'}^{a}(R_{ss'}+\\gamma v_k(s'))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1., 0., 0., 0.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.]]),\n",
       " array([ 0.,  0., -1., -2.,  0., -1., -2., -1., -1., -2., -1.,  0., -2.,\n",
       "        -1.,  0.]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def value_iteration(env, theta=0.001, GAMMA=1.0):\n",
    "    V = np.zeros(env.observation_space.n)\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in range(env.observation_space.n):\n",
    "            A = one_step_lookahead(s, V, GAMMA)\n",
    "            best_action_value = np.max(A)\n",
    "            delta = max(delta, np.abs(best_action_value - V[s]))\n",
    "            V[s] = best_action_value\n",
    "        if delta < theta:\n",
    "            break\n",
    "    policy = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "    for s in range(env.observation_space.n):\n",
    "        A = one_step_lookahead(s, V, GAMMA)\n",
    "        best_action = np.argmax(A)\n",
    "        policy[s, best_action] = 1.0\n",
    "    return policy, V\n",
    "\n",
    "value_iteration(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看出同一个环境，Value Iteration的结果和之前Policy Iteration的结果相同；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalized Policy Iteration(GPI)\n",
    "GPI的思想将会贯彻RL始终，任何RL算法的过程都可以看作是一个GPI的过程。GPI描述了policy evaluation和improvement不断交互提升的过程；评估和提升的方法是多样的。\n",
    "![](https://pic.superbed.cn/item/5ca37c563a213b041794e24f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
